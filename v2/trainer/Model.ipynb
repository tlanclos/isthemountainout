{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.6193 - accuracy: 0.8419\n",
      "Epoch 1: val_loss improved from inf to 1.45755, saving model to isthemountainout.best.h5\n",
      "249/249 [==============================] - 113s 417ms/step - loss: 0.6193 - accuracy: 0.8419 - val_loss: 1.4576 - val_accuracy: 0.5089\n",
      "Epoch 2/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.6249 - accuracy: 0.8358\n",
      "Epoch 2: val_loss improved from 1.45755 to 0.55486, saving model to isthemountainout.best.h5\n",
      "249/249 [==============================] - 101s 404ms/step - loss: 0.6249 - accuracy: 0.8358 - val_loss: 0.5549 - val_accuracy: 0.8808\n",
      "Epoch 3/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.6195 - accuracy: 0.8314\n",
      "Epoch 3: val_loss did not improve from 0.55486\n",
      "249/249 [==============================] - 101s 402ms/step - loss: 0.6195 - accuracy: 0.8314 - val_loss: 1.0924 - val_accuracy: 0.5314\n",
      "Epoch 4/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.6185 - accuracy: 0.8286\n",
      "Epoch 4: val_loss improved from 0.55486 to 0.50953, saving model to isthemountainout.best.h5\n",
      "249/249 [==============================] - 101s 404ms/step - loss: 0.6185 - accuracy: 0.8286 - val_loss: 0.5095 - val_accuracy: 0.8969\n",
      "Epoch 5/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5959 - accuracy: 0.8423\n",
      "Epoch 5: val_loss improved from 0.50953 to 0.48795, saving model to isthemountainout.best.h5\n",
      "249/249 [==============================] - 101s 404ms/step - loss: 0.5959 - accuracy: 0.8423 - val_loss: 0.4880 - val_accuracy: 0.9163\n",
      "Epoch 6/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5971 - accuracy: 0.8423\n",
      "Epoch 6: val_loss did not improve from 0.48795\n",
      "249/249 [==============================] - 101s 403ms/step - loss: 0.5971 - accuracy: 0.8423 - val_loss: 0.6686 - val_accuracy: 0.7971\n",
      "Epoch 7/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5771 - accuracy: 0.8559\n",
      "Epoch 7: val_loss improved from 0.48795 to 0.47242, saving model to isthemountainout.best.h5\n",
      "249/249 [==============================] - 101s 404ms/step - loss: 0.5771 - accuracy: 0.8559 - val_loss: 0.4724 - val_accuracy: 0.9227\n",
      "Epoch 8/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5673 - accuracy: 0.8579\n",
      "Epoch 8: val_loss did not improve from 0.47242\n",
      "249/249 [==============================] - 101s 403ms/step - loss: 0.5673 - accuracy: 0.8579 - val_loss: 0.4764 - val_accuracy: 0.9082\n",
      "Epoch 9/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5610 - accuracy: 0.8575\n",
      "Epoch 9: val_loss improved from 0.47242 to 0.45423, saving model to isthemountainout.best.h5\n",
      "249/249 [==============================] - 101s 404ms/step - loss: 0.5610 - accuracy: 0.8575 - val_loss: 0.4542 - val_accuracy: 0.9163\n",
      "Epoch 10/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5560 - accuracy: 0.8612\n",
      "Epoch 10: val_loss improved from 0.45423 to 0.45261, saving model to isthemountainout.best.h5\n",
      "249/249 [==============================] - 101s 404ms/step - loss: 0.5560 - accuracy: 0.8612 - val_loss: 0.4526 - val_accuracy: 0.9227\n",
      "Epoch 11/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5992 - accuracy: 0.8406\n",
      "Epoch 11: val_loss did not improve from 0.45261\n",
      "249/249 [==============================] - 101s 403ms/step - loss: 0.5992 - accuracy: 0.8406 - val_loss: 0.6088 - val_accuracy: 0.8583\n",
      "Epoch 12/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5984 - accuracy: 0.8487\n",
      "Epoch 12: val_loss did not improve from 0.45261\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5984 - accuracy: 0.8487 - val_loss: 0.7905 - val_accuracy: 0.6812\n",
      "Epoch 13/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.6069 - accuracy: 0.8302\n",
      "Epoch 13: val_loss did not improve from 0.45261\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.6069 - accuracy: 0.8302 - val_loss: 0.5515 - val_accuracy: 0.8824\n",
      "Epoch 14/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5991 - accuracy: 0.8503\n",
      "Epoch 14: val_loss did not improve from 0.45261\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5991 - accuracy: 0.8503 - val_loss: 0.5202 - val_accuracy: 0.8841\n",
      "Epoch 15/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5949 - accuracy: 0.8495\n",
      "Epoch 15: val_loss did not improve from 0.45261\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5949 - accuracy: 0.8495 - val_loss: 2.0072 - val_accuracy: 0.5040\n",
      "Epoch 16/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5890 - accuracy: 0.8588\n",
      "Epoch 16: val_loss did not improve from 0.45261\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5890 - accuracy: 0.8588 - val_loss: 0.6360 - val_accuracy: 0.8502\n",
      "Epoch 17/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5759 - accuracy: 0.8559\n",
      "Epoch 17: val_loss did not improve from 0.45261\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5759 - accuracy: 0.8559 - val_loss: 0.5286 - val_accuracy: 0.8889\n",
      "Epoch 18/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5708 - accuracy: 0.8539\n",
      "Epoch 18: val_loss did not improve from 0.45261\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5708 - accuracy: 0.8539 - val_loss: 0.5011 - val_accuracy: 0.9018\n",
      "Epoch 19/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5794 - accuracy: 0.8575\n",
      "Epoch 19: val_loss did not improve from 0.45261\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5794 - accuracy: 0.8575 - val_loss: 0.6652 - val_accuracy: 0.8406\n",
      "Epoch 20/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5762 - accuracy: 0.8499\n",
      "Epoch 20: val_loss did not improve from 0.45261\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5762 - accuracy: 0.8499 - val_loss: 0.4696 - val_accuracy: 0.9114\n",
      "Epoch 21/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5618 - accuracy: 0.8636\n",
      "Epoch 21: val_loss did not improve from 0.45261\n",
      "249/249 [==============================] - 101s 402ms/step - loss: 0.5618 - accuracy: 0.8636 - val_loss: 0.4831 - val_accuracy: 0.9018\n",
      "Epoch 22/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5523 - accuracy: 0.8551\n",
      "Epoch 22: val_loss improved from 0.45261 to 0.43919, saving model to isthemountainout.best.h5\n",
      "249/249 [==============================] - 100s 402ms/step - loss: 0.5523 - accuracy: 0.8551 - val_loss: 0.4392 - val_accuracy: 0.9291\n",
      "Epoch 23/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5495 - accuracy: 0.8616\n",
      "Epoch 23: val_loss did not improve from 0.43919\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5495 - accuracy: 0.8616 - val_loss: 0.4659 - val_accuracy: 0.9179\n",
      "Epoch 24/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5393 - accuracy: 0.8724\n",
      "Epoch 24: val_loss did not improve from 0.43919\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5393 - accuracy: 0.8724 - val_loss: 0.4560 - val_accuracy: 0.9195\n",
      "Epoch 25/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5363 - accuracy: 0.8801\n",
      "Epoch 25: val_loss improved from 0.43919 to 0.43474, saving model to isthemountainout.best.h5\n",
      "249/249 [==============================] - 101s 402ms/step - loss: 0.5363 - accuracy: 0.8801 - val_loss: 0.4347 - val_accuracy: 0.9324\n",
      "Epoch 26/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5328 - accuracy: 0.8708\n",
      "Epoch 26: val_loss did not improve from 0.43474\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5328 - accuracy: 0.8708 - val_loss: 0.4353 - val_accuracy: 0.9291\n",
      "Epoch 27/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5360 - accuracy: 0.8700\n",
      "Epoch 27: val_loss did not improve from 0.43474\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5360 - accuracy: 0.8700 - val_loss: 0.4372 - val_accuracy: 0.9259\n",
      "Epoch 28/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5284 - accuracy: 0.8757\n",
      "Epoch 28: val_loss did not improve from 0.43474\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5284 - accuracy: 0.8757 - val_loss: 0.4394 - val_accuracy: 0.9259\n",
      "Epoch 29/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5092 - accuracy: 0.8893\n",
      "Epoch 29: val_loss improved from 0.43474 to 0.43449, saving model to isthemountainout.best.h5\n",
      "249/249 [==============================] - 100s 402ms/step - loss: 0.5092 - accuracy: 0.8893 - val_loss: 0.4345 - val_accuracy: 0.9291\n",
      "Epoch 30/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5354 - accuracy: 0.8700\n",
      "Epoch 30: val_loss did not improve from 0.43449\n",
      "249/249 [==============================] - 102s 408ms/step - loss: 0.5354 - accuracy: 0.8700 - val_loss: 0.4351 - val_accuracy: 0.9291\n",
      "Epoch 31/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5748 - accuracy: 0.8628\n",
      "Epoch 31: val_loss did not improve from 0.43449\n",
      "249/249 [==============================] - 101s 404ms/step - loss: 0.5748 - accuracy: 0.8628 - val_loss: 0.6697 - val_accuracy: 0.7858\n",
      "Epoch 32/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5762 - accuracy: 0.8571\n",
      "Epoch 32: val_loss did not improve from 0.43449\n",
      "249/249 [==============================] - 101s 403ms/step - loss: 0.5762 - accuracy: 0.8571 - val_loss: 0.5690 - val_accuracy: 0.8776\n",
      "Epoch 33/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5744 - accuracy: 0.8664\n",
      "Epoch 33: val_loss did not improve from 0.43449\n",
      "249/249 [==============================] - 101s 402ms/step - loss: 0.5744 - accuracy: 0.8664 - val_loss: 0.5913 - val_accuracy: 0.8760\n",
      "Epoch 34/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5562 - accuracy: 0.8716\n",
      "Epoch 34: val_loss did not improve from 0.43449\n",
      "249/249 [==============================] - 101s 403ms/step - loss: 0.5562 - accuracy: 0.8716 - val_loss: 0.5226 - val_accuracy: 0.9147\n",
      "Epoch 35/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5624 - accuracy: 0.8668\n",
      "Epoch 35: val_loss did not improve from 0.43449\n",
      "249/249 [==============================] - 101s 402ms/step - loss: 0.5624 - accuracy: 0.8668 - val_loss: 0.5153 - val_accuracy: 0.9163\n",
      "Epoch 36/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5637 - accuracy: 0.8660\n",
      "Epoch 36: val_loss did not improve from 0.43449\n",
      "249/249 [==============================] - 101s 402ms/step - loss: 0.5637 - accuracy: 0.8660 - val_loss: 0.4732 - val_accuracy: 0.9211\n",
      "Epoch 37/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5603 - accuracy: 0.8660\n",
      "Epoch 37: val_loss did not improve from 0.43449\n",
      "249/249 [==============================] - 101s 402ms/step - loss: 0.5603 - accuracy: 0.8660 - val_loss: 0.4754 - val_accuracy: 0.9147\n",
      "Epoch 38/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5521 - accuracy: 0.8688\n",
      "Epoch 38: val_loss did not improve from 0.43449\n",
      "249/249 [==============================] - 101s 403ms/step - loss: 0.5521 - accuracy: 0.8688 - val_loss: 0.6350 - val_accuracy: 0.8712\n",
      "Epoch 39/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5545 - accuracy: 0.8761\n",
      "Epoch 39: val_loss did not improve from 0.43449\n",
      "249/249 [==============================] - 101s 403ms/step - loss: 0.5545 - accuracy: 0.8761 - val_loss: 1.3689 - val_accuracy: 0.5137\n",
      "Epoch 40/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5457 - accuracy: 0.8684\n",
      "Epoch 40: val_loss did not improve from 0.43449\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5457 - accuracy: 0.8684 - val_loss: 0.4615 - val_accuracy: 0.9340\n",
      "Epoch 41/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5552 - accuracy: 0.8668\n",
      "Epoch 41: val_loss did not improve from 0.43449\n",
      "249/249 [==============================] - 100s 402ms/step - loss: 0.5552 - accuracy: 0.8668 - val_loss: 0.5508 - val_accuracy: 0.9259\n",
      "Epoch 42/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5392 - accuracy: 0.8757\n",
      "Epoch 42: val_loss improved from 0.43449 to 0.41288, saving model to isthemountainout.best.h5\n",
      "249/249 [==============================] - 100s 402ms/step - loss: 0.5392 - accuracy: 0.8757 - val_loss: 0.4129 - val_accuracy: 0.9404\n",
      "Epoch 43/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5386 - accuracy: 0.8781\n",
      "Epoch 43: val_loss did not improve from 0.41288\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5386 - accuracy: 0.8781 - val_loss: 0.4803 - val_accuracy: 0.9163\n",
      "Epoch 44/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5416 - accuracy: 0.8724\n",
      "Epoch 44: val_loss did not improve from 0.41288\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5416 - accuracy: 0.8724 - val_loss: 0.4322 - val_accuracy: 0.9372\n",
      "Epoch 45/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5252 - accuracy: 0.8825\n",
      "Epoch 45: val_loss did not improve from 0.41288\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5252 - accuracy: 0.8825 - val_loss: 0.4488 - val_accuracy: 0.9372\n",
      "Epoch 46/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5258 - accuracy: 0.8732\n",
      "Epoch 46: val_loss did not improve from 0.41288\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5258 - accuracy: 0.8732 - val_loss: 0.4555 - val_accuracy: 0.9372\n",
      "Epoch 47/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5311 - accuracy: 0.8688\n",
      "Epoch 47: val_loss did not improve from 0.41288\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5311 - accuracy: 0.8688 - val_loss: 0.4749 - val_accuracy: 0.9195\n",
      "Epoch 48/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5326 - accuracy: 0.8744\n",
      "Epoch 48: val_loss did not improve from 0.41288\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5326 - accuracy: 0.8744 - val_loss: 0.6559 - val_accuracy: 0.8164\n",
      "Epoch 49/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5316 - accuracy: 0.8773\n",
      "Epoch 49: val_loss improved from 0.41288 to 0.40598, saving model to isthemountainout.best.h5\n",
      "249/249 [==============================] - 101s 402ms/step - loss: 0.5316 - accuracy: 0.8773 - val_loss: 0.4060 - val_accuracy: 0.9404\n",
      "Epoch 50/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5189 - accuracy: 0.8869\n",
      "Epoch 50: val_loss did not improve from 0.40598\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5189 - accuracy: 0.8869 - val_loss: 0.4528 - val_accuracy: 0.9275\n",
      "Epoch 51/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5144 - accuracy: 0.8922\n",
      "Epoch 51: val_loss did not improve from 0.40598\n",
      "249/249 [==============================] - 101s 402ms/step - loss: 0.5144 - accuracy: 0.8922 - val_loss: 0.4138 - val_accuracy: 0.9404\n",
      "Epoch 52/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5167 - accuracy: 0.8849\n",
      "Epoch 52: val_loss did not improve from 0.40598\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5167 - accuracy: 0.8849 - val_loss: 0.4339 - val_accuracy: 0.9420\n",
      "Epoch 53/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5103 - accuracy: 0.8901\n",
      "Epoch 53: val_loss did not improve from 0.40598\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5103 - accuracy: 0.8901 - val_loss: 0.4152 - val_accuracy: 0.9469\n",
      "Epoch 54/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5256 - accuracy: 0.8748\n",
      "Epoch 54: val_loss did not improve from 0.40598\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5256 - accuracy: 0.8748 - val_loss: 0.4130 - val_accuracy: 0.9436\n",
      "Epoch 55/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5151 - accuracy: 0.8797\n",
      "Epoch 55: val_loss improved from 0.40598 to 0.39979, saving model to isthemountainout.best.h5\n",
      "249/249 [==============================] - 100s 402ms/step - loss: 0.5151 - accuracy: 0.8797 - val_loss: 0.3998 - val_accuracy: 0.9485\n",
      "Epoch 56/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5059 - accuracy: 0.8881\n",
      "Epoch 56: val_loss did not improve from 0.39979\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5059 - accuracy: 0.8881 - val_loss: 0.4006 - val_accuracy: 0.9501\n",
      "Epoch 57/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5209 - accuracy: 0.8797\n",
      "Epoch 57: val_loss did not improve from 0.39979\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5209 - accuracy: 0.8797 - val_loss: 0.4022 - val_accuracy: 0.9533\n",
      "Epoch 58/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5090 - accuracy: 0.8869\n",
      "Epoch 58: val_loss improved from 0.39979 to 0.38949, saving model to isthemountainout.best.h5\n",
      "249/249 [==============================] - 100s 402ms/step - loss: 0.5090 - accuracy: 0.8869 - val_loss: 0.3895 - val_accuracy: 0.9565\n",
      "Epoch 59/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5076 - accuracy: 0.8922\n",
      "Epoch 59: val_loss did not improve from 0.38949\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5076 - accuracy: 0.8922 - val_loss: 0.3956 - val_accuracy: 0.9485\n",
      "Epoch 60/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5120 - accuracy: 0.8833\n",
      "Epoch 60: val_loss improved from 0.38949 to 0.38600, saving model to isthemountainout.best.h5\n",
      "249/249 [==============================] - 101s 402ms/step - loss: 0.5120 - accuracy: 0.8833 - val_loss: 0.3860 - val_accuracy: 0.9469\n",
      "Epoch 61/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5050 - accuracy: 0.8905\n",
      "Epoch 61: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 101s 402ms/step - loss: 0.5050 - accuracy: 0.8905 - val_loss: 0.3906 - val_accuracy: 0.9501\n",
      "Epoch 62/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4975 - accuracy: 0.8930\n",
      "Epoch 62: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.4975 - accuracy: 0.8930 - val_loss: 0.3905 - val_accuracy: 0.9517\n",
      "Epoch 63/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4926 - accuracy: 0.8966\n",
      "Epoch 63: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.4926 - accuracy: 0.8966 - val_loss: 0.3878 - val_accuracy: 0.9501\n",
      "Epoch 64/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4982 - accuracy: 0.8897\n",
      "Epoch 64: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.4982 - accuracy: 0.8897 - val_loss: 0.3862 - val_accuracy: 0.9501\n",
      "Epoch 65/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5000 - accuracy: 0.8982\n",
      "Epoch 65: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5000 - accuracy: 0.8982 - val_loss: 0.3866 - val_accuracy: 0.9501\n",
      "Epoch 66/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4939 - accuracy: 0.8909\n",
      "Epoch 66: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.4939 - accuracy: 0.8909 - val_loss: 0.3908 - val_accuracy: 0.9485\n",
      "Epoch 67/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4891 - accuracy: 0.9014\n",
      "Epoch 67: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.4891 - accuracy: 0.9014 - val_loss: 0.3900 - val_accuracy: 0.9420\n",
      "Epoch 68/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4913 - accuracy: 0.8966\n",
      "Epoch 68: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.4913 - accuracy: 0.8966 - val_loss: 0.3900 - val_accuracy: 0.9420\n",
      "Epoch 69/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4803 - accuracy: 0.8998\n",
      "Epoch 69: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.4803 - accuracy: 0.8998 - val_loss: 0.3905 - val_accuracy: 0.9420\n",
      "Epoch 70/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4994 - accuracy: 0.8930\n",
      "Epoch 70: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.4994 - accuracy: 0.8930 - val_loss: 0.3906 - val_accuracy: 0.9436\n",
      "Epoch 71/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5348 - accuracy: 0.8781\n",
      "Epoch 71: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 101s 402ms/step - loss: 0.5348 - accuracy: 0.8781 - val_loss: 0.4901 - val_accuracy: 0.9275\n",
      "Epoch 72/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5338 - accuracy: 0.8712\n",
      "Epoch 72: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5338 - accuracy: 0.8712 - val_loss: 0.5263 - val_accuracy: 0.9388\n",
      "Epoch 73/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5364 - accuracy: 0.8720\n",
      "Epoch 73: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5364 - accuracy: 0.8720 - val_loss: 0.8503 - val_accuracy: 0.7536\n",
      "Epoch 74/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5398 - accuracy: 0.8648\n",
      "Epoch 74: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5398 - accuracy: 0.8648 - val_loss: 0.4784 - val_accuracy: 0.9195\n",
      "Epoch 75/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5337 - accuracy: 0.8813\n",
      "Epoch 75: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5337 - accuracy: 0.8813 - val_loss: 0.4348 - val_accuracy: 0.9388\n",
      "Epoch 76/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5408 - accuracy: 0.8765\n",
      "Epoch 76: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5408 - accuracy: 0.8765 - val_loss: 0.4580 - val_accuracy: 0.9243\n",
      "Epoch 77/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5285 - accuracy: 0.8753\n",
      "Epoch 77: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5285 - accuracy: 0.8753 - val_loss: 0.5071 - val_accuracy: 0.9372\n",
      "Epoch 78/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5313 - accuracy: 0.8704\n",
      "Epoch 78: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5313 - accuracy: 0.8704 - val_loss: 0.4196 - val_accuracy: 0.9436\n",
      "Epoch 79/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5311 - accuracy: 0.8789\n",
      "Epoch 79: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5311 - accuracy: 0.8789 - val_loss: 0.4393 - val_accuracy: 0.9452\n",
      "Epoch 80/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5323 - accuracy: 0.8813\n",
      "Epoch 80: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5323 - accuracy: 0.8813 - val_loss: 0.5493 - val_accuracy: 0.9130\n",
      "Epoch 81/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5330 - accuracy: 0.8748\n",
      "Epoch 81: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 101s 402ms/step - loss: 0.5330 - accuracy: 0.8748 - val_loss: 0.5585 - val_accuracy: 0.9098\n",
      "Epoch 82/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5200 - accuracy: 0.8861\n",
      "Epoch 82: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5200 - accuracy: 0.8861 - val_loss: 0.6495 - val_accuracy: 0.8293\n",
      "Epoch 83/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5234 - accuracy: 0.8712\n",
      "Epoch 83: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5234 - accuracy: 0.8712 - val_loss: 0.4324 - val_accuracy: 0.9404\n",
      "Epoch 84/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5403 - accuracy: 0.8684\n",
      "Epoch 84: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5403 - accuracy: 0.8684 - val_loss: 0.4396 - val_accuracy: 0.9340\n",
      "Epoch 85/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5214 - accuracy: 0.8821\n",
      "Epoch 85: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5214 - accuracy: 0.8821 - val_loss: 0.7735 - val_accuracy: 0.7279\n",
      "Epoch 86/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5310 - accuracy: 0.8781\n",
      "Epoch 86: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5310 - accuracy: 0.8781 - val_loss: 0.4153 - val_accuracy: 0.9452\n",
      "Epoch 87/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5169 - accuracy: 0.8773\n",
      "Epoch 87: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5169 - accuracy: 0.8773 - val_loss: 0.4744 - val_accuracy: 0.9324\n",
      "Epoch 88/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5299 - accuracy: 0.8736\n",
      "Epoch 88: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5299 - accuracy: 0.8736 - val_loss: 0.5370 - val_accuracy: 0.9130\n",
      "Epoch 89/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5280 - accuracy: 0.8825\n",
      "Epoch 89: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5280 - accuracy: 0.8825 - val_loss: 0.4156 - val_accuracy: 0.9501\n",
      "Epoch 90/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5269 - accuracy: 0.8753\n",
      "Epoch 90: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5269 - accuracy: 0.8753 - val_loss: 0.4771 - val_accuracy: 0.9308\n",
      "Epoch 91/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5201 - accuracy: 0.8825\n",
      "Epoch 91: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 101s 402ms/step - loss: 0.5201 - accuracy: 0.8825 - val_loss: 0.4032 - val_accuracy: 0.9420\n",
      "Epoch 92/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5157 - accuracy: 0.8861\n",
      "Epoch 92: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5157 - accuracy: 0.8861 - val_loss: 0.4212 - val_accuracy: 0.9388\n",
      "Epoch 93/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5130 - accuracy: 0.8897\n",
      "Epoch 93: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5130 - accuracy: 0.8897 - val_loss: 0.4530 - val_accuracy: 0.9356\n",
      "Epoch 94/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5103 - accuracy: 0.8885\n",
      "Epoch 94: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5103 - accuracy: 0.8885 - val_loss: 0.5114 - val_accuracy: 0.9340\n",
      "Epoch 95/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5122 - accuracy: 0.8825\n",
      "Epoch 95: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 101s 403ms/step - loss: 0.5122 - accuracy: 0.8825 - val_loss: 0.4086 - val_accuracy: 0.9452\n",
      "Epoch 96/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5101 - accuracy: 0.8861\n",
      "Epoch 96: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5101 - accuracy: 0.8861 - val_loss: 0.4108 - val_accuracy: 0.9420\n",
      "Epoch 97/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5084 - accuracy: 0.8901\n",
      "Epoch 97: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5084 - accuracy: 0.8901 - val_loss: 0.4027 - val_accuracy: 0.9517\n",
      "Epoch 98/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5059 - accuracy: 0.8821\n",
      "Epoch 98: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 402ms/step - loss: 0.5059 - accuracy: 0.8821 - val_loss: 0.4446 - val_accuracy: 0.9420\n",
      "Epoch 99/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5179 - accuracy: 0.8797\n",
      "Epoch 99: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5179 - accuracy: 0.8797 - val_loss: 0.4559 - val_accuracy: 0.9404\n",
      "Epoch 100/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5206 - accuracy: 0.8769\n",
      "Epoch 100: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5206 - accuracy: 0.8769 - val_loss: 0.4098 - val_accuracy: 0.9501\n",
      "Epoch 101/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5108 - accuracy: 0.8881\n",
      "Epoch 101: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 402ms/step - loss: 0.5108 - accuracy: 0.8881 - val_loss: 0.4064 - val_accuracy: 0.9452\n",
      "Epoch 102/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5114 - accuracy: 0.8873\n",
      "Epoch 102: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5114 - accuracy: 0.8873 - val_loss: 0.4444 - val_accuracy: 0.9420\n",
      "Epoch 103/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5042 - accuracy: 0.8857\n",
      "Epoch 103: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 106s 425ms/step - loss: 0.5042 - accuracy: 0.8857 - val_loss: 0.4060 - val_accuracy: 0.9436\n",
      "Epoch 104/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5045 - accuracy: 0.8889\n",
      "Epoch 104: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 106s 423ms/step - loss: 0.5045 - accuracy: 0.8889 - val_loss: 0.4257 - val_accuracy: 0.9291\n",
      "Epoch 105/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5066 - accuracy: 0.8905\n",
      "Epoch 105: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 106s 425ms/step - loss: 0.5066 - accuracy: 0.8905 - val_loss: 0.4298 - val_accuracy: 0.9420\n",
      "Epoch 106/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5041 - accuracy: 0.8805\n",
      "Epoch 106: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 108s 430ms/step - loss: 0.5041 - accuracy: 0.8805 - val_loss: 0.4004 - val_accuracy: 0.9469\n",
      "Epoch 107/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4999 - accuracy: 0.8950\n",
      "Epoch 107: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 106s 422ms/step - loss: 0.4999 - accuracy: 0.8950 - val_loss: 0.3943 - val_accuracy: 0.9420\n",
      "Epoch 108/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4976 - accuracy: 0.8970\n",
      "Epoch 108: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 103s 412ms/step - loss: 0.4976 - accuracy: 0.8970 - val_loss: 0.4460 - val_accuracy: 0.9372\n",
      "Epoch 109/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4971 - accuracy: 0.9014\n",
      "Epoch 109: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 103s 413ms/step - loss: 0.4971 - accuracy: 0.9014 - val_loss: 0.4274 - val_accuracy: 0.9469\n",
      "Epoch 110/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5078 - accuracy: 0.8861\n",
      "Epoch 110: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 103s 411ms/step - loss: 0.5078 - accuracy: 0.8861 - val_loss: 0.3975 - val_accuracy: 0.9420\n",
      "Epoch 111/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4890 - accuracy: 0.8938\n",
      "Epoch 111: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 104s 418ms/step - loss: 0.4890 - accuracy: 0.8938 - val_loss: 0.4353 - val_accuracy: 0.9308\n",
      "Epoch 112/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4823 - accuracy: 0.9010\n",
      "Epoch 112: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 109s 435ms/step - loss: 0.4823 - accuracy: 0.9010 - val_loss: 0.4180 - val_accuracy: 0.9340\n",
      "Epoch 113/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5008 - accuracy: 0.8913\n",
      "Epoch 113: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 106s 425ms/step - loss: 0.5008 - accuracy: 0.8913 - val_loss: 0.4744 - val_accuracy: 0.9452\n",
      "Epoch 114/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4895 - accuracy: 0.8994\n",
      "Epoch 114: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 103s 412ms/step - loss: 0.4895 - accuracy: 0.8994 - val_loss: 0.4121 - val_accuracy: 0.9436\n",
      "Epoch 115/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4839 - accuracy: 0.8942\n",
      "Epoch 115: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 103s 412ms/step - loss: 0.4839 - accuracy: 0.8942 - val_loss: 0.3983 - val_accuracy: 0.9501\n",
      "Epoch 116/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4902 - accuracy: 0.8938\n",
      "Epoch 116: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 103s 412ms/step - loss: 0.4902 - accuracy: 0.8938 - val_loss: 0.3873 - val_accuracy: 0.9533\n",
      "Epoch 117/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4813 - accuracy: 0.9030\n",
      "Epoch 117: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 104s 415ms/step - loss: 0.4813 - accuracy: 0.9030 - val_loss: 0.3942 - val_accuracy: 0.9452\n",
      "Epoch 118/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4888 - accuracy: 0.8978\n",
      "Epoch 118: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 103s 412ms/step - loss: 0.4888 - accuracy: 0.8978 - val_loss: 0.4211 - val_accuracy: 0.9469\n",
      "Epoch 119/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4885 - accuracy: 0.8922\n",
      "Epoch 119: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 103s 412ms/step - loss: 0.4885 - accuracy: 0.8922 - val_loss: 0.4007 - val_accuracy: 0.9485\n",
      "Epoch 120/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4821 - accuracy: 0.9066\n",
      "Epoch 120: val_loss improved from 0.38600 to 0.38300, saving model to isthemountainout.best.h5\n",
      "249/249 [==============================] - 104s 414ms/step - loss: 0.4821 - accuracy: 0.9066 - val_loss: 0.3830 - val_accuracy: 0.9485\n",
      "Epoch 121/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4797 - accuracy: 0.9014\n",
      "Epoch 121: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 104s 414ms/step - loss: 0.4797 - accuracy: 0.9014 - val_loss: 0.3899 - val_accuracy: 0.9485\n",
      "Epoch 122/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4860 - accuracy: 0.9066\n",
      "Epoch 122: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 103s 412ms/step - loss: 0.4860 - accuracy: 0.9066 - val_loss: 0.3917 - val_accuracy: 0.9485\n",
      "Epoch 123/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4846 - accuracy: 0.8974\n",
      "Epoch 123: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 102s 408ms/step - loss: 0.4846 - accuracy: 0.8974 - val_loss: 0.4032 - val_accuracy: 0.9549\n",
      "Epoch 124/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4847 - accuracy: 0.8974\n",
      "Epoch 124: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.4847 - accuracy: 0.8974 - val_loss: 0.3884 - val_accuracy: 0.9533\n",
      "Epoch 125/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4825 - accuracy: 0.9022\n",
      "Epoch 125: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.4825 - accuracy: 0.9022 - val_loss: 0.3957 - val_accuracy: 0.9517\n",
      "Epoch 126/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4790 - accuracy: 0.9054\n",
      "Epoch 126: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.4790 - accuracy: 0.9054 - val_loss: 0.3886 - val_accuracy: 0.9517\n",
      "Epoch 127/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4797 - accuracy: 0.9082\n",
      "Epoch 127: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.4797 - accuracy: 0.9082 - val_loss: 0.3944 - val_accuracy: 0.9485\n",
      "Epoch 128/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4726 - accuracy: 0.9070\n",
      "Epoch 128: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.4726 - accuracy: 0.9070 - val_loss: 0.3859 - val_accuracy: 0.9517\n",
      "Epoch 129/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4666 - accuracy: 0.9042\n",
      "Epoch 129: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.4666 - accuracy: 0.9042 - val_loss: 0.3897 - val_accuracy: 0.9469\n",
      "Epoch 130/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4791 - accuracy: 0.9026\n",
      "Epoch 130: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 100s 402ms/step - loss: 0.4791 - accuracy: 0.9026 - val_loss: 0.3903 - val_accuracy: 0.9469\n",
      "Epoch 131/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4781 - accuracy: 0.9014\n",
      "Epoch 131: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 101s 402ms/step - loss: 0.4781 - accuracy: 0.9014 - val_loss: 0.3904 - val_accuracy: 0.9501\n",
      "Epoch 132/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4715 - accuracy: 0.9091\n",
      "Epoch 132: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.4715 - accuracy: 0.9091 - val_loss: 0.3876 - val_accuracy: 0.9517\n",
      "Epoch 133/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4658 - accuracy: 0.9107\n",
      "Epoch 133: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.4658 - accuracy: 0.9107 - val_loss: 0.3899 - val_accuracy: 0.9549\n",
      "Epoch 134/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4691 - accuracy: 0.9074\n",
      "Epoch 134: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.4691 - accuracy: 0.9074 - val_loss: 0.3929 - val_accuracy: 0.9549\n",
      "Epoch 135/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4665 - accuracy: 0.9010\n",
      "Epoch 135: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.4665 - accuracy: 0.9010 - val_loss: 0.3944 - val_accuracy: 0.9517\n",
      "Epoch 136/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4752 - accuracy: 0.9046\n",
      "Epoch 136: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.4752 - accuracy: 0.9046 - val_loss: 0.3947 - val_accuracy: 0.9517\n",
      "Epoch 137/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4664 - accuracy: 0.9074\n",
      "Epoch 137: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.4664 - accuracy: 0.9074 - val_loss: 0.3914 - val_accuracy: 0.9517\n",
      "Epoch 138/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4695 - accuracy: 0.9050\n",
      "Epoch 138: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.4695 - accuracy: 0.9050 - val_loss: 0.3920 - val_accuracy: 0.9485\n",
      "Epoch 139/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4745 - accuracy: 0.9070\n",
      "Epoch 139: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.4745 - accuracy: 0.9070 - val_loss: 0.3945 - val_accuracy: 0.9549\n",
      "Epoch 140/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4658 - accuracy: 0.9123\n",
      "Epoch 140: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.4658 - accuracy: 0.9123 - val_loss: 0.3954 - val_accuracy: 0.9517\n",
      "Epoch 141/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4742 - accuracy: 0.9014\n",
      "Epoch 141: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 101s 404ms/step - loss: 0.4742 - accuracy: 0.9014 - val_loss: 0.3931 - val_accuracy: 0.9517\n",
      "Epoch 142/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4645 - accuracy: 0.9038\n",
      "Epoch 142: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 101s 402ms/step - loss: 0.4645 - accuracy: 0.9038 - val_loss: 0.3970 - val_accuracy: 0.9517\n",
      "Epoch 143/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4721 - accuracy: 0.9095\n",
      "Epoch 143: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 100s 402ms/step - loss: 0.4721 - accuracy: 0.9095 - val_loss: 0.3971 - val_accuracy: 0.9485\n",
      "Epoch 144/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4695 - accuracy: 0.9058\n",
      "Epoch 144: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 100s 402ms/step - loss: 0.4695 - accuracy: 0.9058 - val_loss: 0.3962 - val_accuracy: 0.9501\n",
      "Epoch 145/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4752 - accuracy: 0.8978\n",
      "Epoch 145: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 101s 402ms/step - loss: 0.4752 - accuracy: 0.8978 - val_loss: 0.3958 - val_accuracy: 0.9517\n",
      "Epoch 146/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4649 - accuracy: 0.9139\n",
      "Epoch 146: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 103s 411ms/step - loss: 0.4649 - accuracy: 0.9139 - val_loss: 0.3955 - val_accuracy: 0.9485\n",
      "Epoch 147/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4690 - accuracy: 0.8986\n",
      "Epoch 147: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 104s 414ms/step - loss: 0.4690 - accuracy: 0.8986 - val_loss: 0.3956 - val_accuracy: 0.9501\n",
      "Epoch 148/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4680 - accuracy: 0.9034\n",
      "Epoch 148: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 104s 418ms/step - loss: 0.4680 - accuracy: 0.9034 - val_loss: 0.3953 - val_accuracy: 0.9501\n",
      "Epoch 149/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4563 - accuracy: 0.9070\n",
      "Epoch 149: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 103s 414ms/step - loss: 0.4563 - accuracy: 0.9070 - val_loss: 0.3958 - val_accuracy: 0.9485\n",
      "Epoch 150/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4695 - accuracy: 0.9103\n",
      "Epoch 150: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 104s 416ms/step - loss: 0.4695 - accuracy: 0.9103 - val_loss: 0.3952 - val_accuracy: 0.9501\n",
      "Epoch 151/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5140 - accuracy: 0.8881\n",
      "Epoch 151: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 104s 414ms/step - loss: 0.5140 - accuracy: 0.8881 - val_loss: 0.4237 - val_accuracy: 0.9420\n",
      "Epoch 152/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5030 - accuracy: 0.8926\n",
      "Epoch 152: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 106s 424ms/step - loss: 0.5030 - accuracy: 0.8926 - val_loss: 0.4380 - val_accuracy: 0.9372\n",
      "Epoch 153/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5136 - accuracy: 0.8893\n",
      "Epoch 153: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 105s 418ms/step - loss: 0.5136 - accuracy: 0.8893 - val_loss: 0.4897 - val_accuracy: 0.9356\n",
      "Epoch 154/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5172 - accuracy: 0.8809\n",
      "Epoch 154: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 105s 419ms/step - loss: 0.5172 - accuracy: 0.8809 - val_loss: 0.5658 - val_accuracy: 0.8921\n",
      "Epoch 155/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5103 - accuracy: 0.8881\n",
      "Epoch 155: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 106s 424ms/step - loss: 0.5103 - accuracy: 0.8881 - val_loss: 0.7429 - val_accuracy: 0.7230\n",
      "Epoch 156/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5059 - accuracy: 0.8845\n",
      "Epoch 156: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 108s 431ms/step - loss: 0.5059 - accuracy: 0.8845 - val_loss: 0.4766 - val_accuracy: 0.9243\n",
      "Epoch 157/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5103 - accuracy: 0.8865\n",
      "Epoch 157: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 109s 436ms/step - loss: 0.5103 - accuracy: 0.8865 - val_loss: 0.3983 - val_accuracy: 0.9501\n",
      "Epoch 158/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5009 - accuracy: 0.8881\n",
      "Epoch 158: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 107s 428ms/step - loss: 0.5009 - accuracy: 0.8881 - val_loss: 0.4717 - val_accuracy: 0.9372\n",
      "Epoch 159/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4981 - accuracy: 0.8885\n",
      "Epoch 159: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 106s 425ms/step - loss: 0.4981 - accuracy: 0.8885 - val_loss: 0.5458 - val_accuracy: 0.8969\n",
      "Epoch 160/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5047 - accuracy: 0.8861\n",
      "Epoch 160: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 104s 418ms/step - loss: 0.5047 - accuracy: 0.8861 - val_loss: 0.5386 - val_accuracy: 0.9259\n",
      "Epoch 161/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5078 - accuracy: 0.8889\n",
      "Epoch 161: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 105s 420ms/step - loss: 0.5078 - accuracy: 0.8889 - val_loss: 0.4615 - val_accuracy: 0.9356\n",
      "Epoch 162/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4991 - accuracy: 0.8877\n",
      "Epoch 162: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 106s 424ms/step - loss: 0.4991 - accuracy: 0.8877 - val_loss: 0.4792 - val_accuracy: 0.9179\n",
      "Epoch 163/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5034 - accuracy: 0.8913\n",
      "Epoch 163: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 105s 422ms/step - loss: 0.5034 - accuracy: 0.8913 - val_loss: 0.4557 - val_accuracy: 0.9436\n",
      "Epoch 164/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5035 - accuracy: 0.8877\n",
      "Epoch 164: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 106s 423ms/step - loss: 0.5035 - accuracy: 0.8877 - val_loss: 0.4063 - val_accuracy: 0.9501\n",
      "Epoch 165/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5007 - accuracy: 0.8909\n",
      "Epoch 165: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 107s 430ms/step - loss: 0.5007 - accuracy: 0.8909 - val_loss: 0.4366 - val_accuracy: 0.9436\n",
      "Epoch 166/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5002 - accuracy: 0.8893\n",
      "Epoch 166: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 109s 437ms/step - loss: 0.5002 - accuracy: 0.8893 - val_loss: 0.4242 - val_accuracy: 0.9469\n",
      "Epoch 167/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5096 - accuracy: 0.8857\n",
      "Epoch 167: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 107s 430ms/step - loss: 0.5096 - accuracy: 0.8857 - val_loss: 0.4405 - val_accuracy: 0.9485\n",
      "Epoch 168/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5020 - accuracy: 0.8897\n",
      "Epoch 168: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 111s 442ms/step - loss: 0.5020 - accuracy: 0.8897 - val_loss: 0.4269 - val_accuracy: 0.9469\n",
      "Epoch 169/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4995 - accuracy: 0.8946\n",
      "Epoch 169: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 110s 438ms/step - loss: 0.4995 - accuracy: 0.8946 - val_loss: 0.4689 - val_accuracy: 0.9340\n",
      "Epoch 170/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5024 - accuracy: 0.8918\n",
      "Epoch 170: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 110s 441ms/step - loss: 0.5024 - accuracy: 0.8918 - val_loss: 0.4027 - val_accuracy: 0.9549\n",
      "Epoch 171/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5086 - accuracy: 0.8825\n",
      "Epoch 171: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 111s 446ms/step - loss: 0.5086 - accuracy: 0.8825 - val_loss: 0.4036 - val_accuracy: 0.9436\n",
      "Epoch 172/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5010 - accuracy: 0.8930\n",
      "Epoch 172: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 111s 444ms/step - loss: 0.5010 - accuracy: 0.8930 - val_loss: 0.4254 - val_accuracy: 0.9420\n",
      "Epoch 173/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4942 - accuracy: 0.8986\n",
      "Epoch 173: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 108s 433ms/step - loss: 0.4942 - accuracy: 0.8986 - val_loss: 0.4603 - val_accuracy: 0.9436\n",
      "Epoch 174/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4969 - accuracy: 0.8930\n",
      "Epoch 174: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 106s 425ms/step - loss: 0.4969 - accuracy: 0.8930 - val_loss: 0.4137 - val_accuracy: 0.9420\n",
      "Epoch 175/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4918 - accuracy: 0.9018\n",
      "Epoch 175: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 107s 428ms/step - loss: 0.4918 - accuracy: 0.9018 - val_loss: 0.4552 - val_accuracy: 0.9404\n",
      "Epoch 176/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4971 - accuracy: 0.8942\n",
      "Epoch 176: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 108s 432ms/step - loss: 0.4971 - accuracy: 0.8942 - val_loss: 0.4131 - val_accuracy: 0.9404\n",
      "Epoch 177/700\n",
      "123/249 [=============>................] - ETA: 51s - loss: 0.4973 - accuracy: 0.8902"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'IteratorGetNext' defined at (most recent call last):\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Taylor\\AppData\\Local\\Temp\\ipykernel_24196\\2578742606.py\", line 23, in <module>\n      model.fit(\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\keras\\engine\\training.py\", line 1145, in step_function\n      data = next(iterator)\nNode: 'IteratorGetNext'\nDetected at node 'IteratorGetNext' defined at (most recent call last):\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Taylor\\AppData\\Local\\Temp\\ipykernel_24196\\2578742606.py\", line 23, in <module>\n      model.fit(\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\keras\\engine\\training.py\", line 1145, in step_function\n      data = next(iterator)\nNode: 'IteratorGetNext'\n2 root error(s) found.\n  (0) RESOURCE_EXHAUSTED:  Failed to allocate memory for the batch of component 0\n\t [[{{node IteratorGetNext}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[model/random_brightness/loop_body/stateful_uniform/Bitcast/pfor/while/body/_378/model/random_brightness/loop_body/stateful_uniform/Bitcast/pfor/while/TensorArrayV2Write/TensorListSetItem/_244]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n  (1) RESOURCE_EXHAUSTED:  Failed to allocate memory for the batch of component 0\n\t [[{{node IteratorGetNext}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_16536]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 23\u001b[0m\n\u001b[0;32m     10\u001b[0m model \u001b[39m=\u001b[39m frozenmodel\u001b[39m.\u001b[39mgenerate_model(\n\u001b[0;32m     11\u001b[0m     weights_filepath\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39misthemountainout.best.h5\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     12\u001b[0m     with_augmentations\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     13\u001b[0m     can_ignore_weights\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m model\u001b[39m.\u001b[39mcompile(\n\u001b[0;32m     15\u001b[0m     optimizer\u001b[39m=\u001b[39moptimizers\u001b[39m.\u001b[39mAdadelta(\n\u001b[0;32m     16\u001b[0m         learning_rate\u001b[39m=\u001b[39moptimizers\u001b[39m.\u001b[39mschedules\u001b[39m.\u001b[39mCosineDecayRestarts(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     22\u001b[0m )\n\u001b[1;32m---> 23\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     24\u001b[0m     dataset\u001b[39m.\u001b[39;49mtraining,\n\u001b[0;32m     25\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m700\u001b[39;49m,\n\u001b[0;32m     26\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     27\u001b[0m     validation_data\u001b[39m=\u001b[39;49mdataset\u001b[39m.\u001b[39;49mvalidation,\n\u001b[0;32m     28\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[\n\u001b[0;32m     29\u001b[0m         callbacks\u001b[39m.\u001b[39;49mTensorBoard(\n\u001b[0;32m     30\u001b[0m             log_dir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlogs/fit/\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m datetime\u001b[39m.\u001b[39;49mnow()\u001b[39m.\u001b[39;49mstrftime(\u001b[39m\"\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mY\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mm\u001b[39;49m\u001b[39m%d\u001b[39;49;00m\u001b[39m-\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mH\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mM\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mS\u001b[39;49m\u001b[39m\"\u001b[39;49m)),\n\u001b[0;32m     31\u001b[0m         tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mcallbacks\u001b[39m.\u001b[39;49mModelCheckpoint(\n\u001b[0;32m     32\u001b[0m             \u001b[39m'\u001b[39;49m\u001b[39misthemountainout.best.h5\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     33\u001b[0m             monitor\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mval_loss\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     34\u001b[0m             mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmin\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     35\u001b[0m             save_best_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     36\u001b[0m             verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m),\n\u001b[0;32m     37\u001b[0m         tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mcallbacks\u001b[39m.\u001b[39;49mEarlyStopping(\n\u001b[0;32m     38\u001b[0m             monitor\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mval_loss\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     39\u001b[0m             mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmin\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     40\u001b[0m             patience\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[0;32m     41\u001b[0m             restore_best_weights\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     42\u001b[0m             verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m),\n\u001b[0;32m     43\u001b[0m         tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mcallbacks\u001b[39m.\u001b[39;49mTensorBoard(\n\u001b[0;32m     44\u001b[0m             log_dir\u001b[39m=\u001b[39;49mos\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(\u001b[39m'\u001b[39;49m\u001b[39mtensor-logs\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mfit\u001b[39;49m\u001b[39m'\u001b[39;49m, datetime\u001b[39m.\u001b[39;49mnow()\u001b[39m.\u001b[39;49mstrftime(\u001b[39m'\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mY\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mm\u001b[39;49m\u001b[39m%d\u001b[39;49;00m\u001b[39m%\u001b[39;49m\u001b[39mH\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mM\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mS\u001b[39;49m\u001b[39m'\u001b[39;49m)),\n\u001b[0;32m     45\u001b[0m             update_freq\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[0;32m     46\u001b[0m             write_images\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     47\u001b[0m             write_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     48\u001b[0m             embeddings_freq\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m),\n\u001b[0;32m     49\u001b[0m     ])\n",
      "File \u001b[1;32mc:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'IteratorGetNext' defined at (most recent call last):\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Taylor\\AppData\\Local\\Temp\\ipykernel_24196\\2578742606.py\", line 23, in <module>\n      model.fit(\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\keras\\engine\\training.py\", line 1145, in step_function\n      data = next(iterator)\nNode: 'IteratorGetNext'\nDetected at node 'IteratorGetNext' defined at (most recent call last):\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Taylor\\AppData\\Local\\Temp\\ipykernel_24196\\2578742606.py\", line 23, in <module>\n      model.fit(\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\keras\\engine\\training.py\", line 1145, in step_function\n      data = next(iterator)\nNode: 'IteratorGetNext'\n2 root error(s) found.\n  (0) RESOURCE_EXHAUSTED:  Failed to allocate memory for the batch of component 0\n\t [[{{node IteratorGetNext}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[model/random_brightness/loop_body/stateful_uniform/Bitcast/pfor/while/body/_378/model/random_brightness/loop_body/stateful_uniform/Bitcast/pfor/while/TensorArrayV2Write/TensorListSetItem/_244]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n  (1) RESOURCE_EXHAUSTED:  Failed to allocate memory for the batch of component 0\n\t [[{{node IteratorGetNext}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_16536]"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "%run setup/GpuOptions.ipynb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "'..' not in sys.path and sys.path.append('..')\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import importlib\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from common import frozenmodel\n",
    "from common.dataset import Dataset\n",
    "from tensorflow.keras import callbacks, optimizers, losses\n",
    "importlib.reload(frozenmodel)\n",
    "\n",
    "dataset = Dataset(\n",
    "    '../dataset/dataset-2022-10-19T22-51-08',\n",
    "    batch_size=10,\n",
    "    image_size=(1080, 1920))\n",
    "\n",
    "model = frozenmodel.generate_model(\n",
    "    weights_filepath='isthemountainout.best.h5',\n",
    "    with_augmentations=True,\n",
    "    can_ignore_weights=True)\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adadelta(\n",
    "        learning_rate=optimizers.schedules.CosineDecayRestarts(\n",
    "            2.5,\n",
    "            # Decay over 5 epochs and then restart \n",
    "            dataset.training.cardinality().numpy() * 10)),\n",
    "    loss=losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.08),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "model.fit(\n",
    "    dataset.training,\n",
    "    epochs=700,\n",
    "    verbose=True,\n",
    "    validation_data=dataset.validation,\n",
    "    callbacks=[\n",
    "        callbacks.TensorBoard(\n",
    "            log_dir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            'isthemountainout.best.h5',\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_best_only=True,\n",
    "            verbose=True),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            patience=100,\n",
    "            restore_best_weights=True,\n",
    "            verbose=True),\n",
    "        tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=os.path.join('tensor-logs', 'fit', datetime.now().strftime('%Y%m%d%H%M%S')),\n",
    "            update_freq=50,\n",
    "            write_images=True,\n",
    "            write_graph=True,\n",
    "            embeddings_freq=10),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n",
      "Enabling memory growth on PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "loading model with best weights\n",
      "Found 3106 files belonging to 4 classes.\n",
      "Using 2175 files for training.\n",
      "Found 3106 files belonging to 4 classes.\n",
      "Using 931 files for validation.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run setup/GpuOptions.ipynb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "'..' not in sys.path and sys.path.append('..')\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import importlib\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from common import frozenmodel\n",
    "from common.dataset import Dataset\n",
    "importlib.reload(frozenmodel)\n",
    "\n",
    "print('loading model with best weights')\n",
    "# from common.weights import weights\n",
    "# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '../isthemountainout-credentials.json'\n",
    "# with weights() as filename:\n",
    "model = frozenmodel.generate_model(\n",
    "    weights_filepath='isthemountainout.h5', # filename\n",
    "    with_augmentations=True)\n",
    "\n",
    "dataset = Dataset(\n",
    "    '../dataset/dataset-2022-10-19T22-51-08',\n",
    "    batch_size=6,\n",
    "    image_size=(1080, 1920),\n",
    "    validation_split=0.3)\n",
    "validations = dataset.validation\n",
    "predictions = []\n",
    "labels = []\n",
    "for data, label in validations:\n",
    "  labels.append(tf.argmax(label, axis=1))\n",
    "  predictions.append(tf.math.argmax(tf.nn.softmax(model.predict(data, verbose=0)), axis=1))\n",
    "labels = tf.concat(labels, axis=0)\n",
    "predictions = tf.concat(predictions, axis=0)\n",
    "cm = confusion_matrix(labels, predictions, labels=list(range(len(dataset.validation.class_names))))\n",
    "cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "sns.heatmap(\n",
    "  cmn,\n",
    "  annot=True,\n",
    "  fmt='.2f',\n",
    "  xticklabels=dataset.validation.class_names,\n",
    "  yticklabels=dataset.validation.class_names)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show(block=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mountain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "675b856ca8a14374dc4009cf40e9ead9d95917e285fd3b96229cb66bb660ef16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
