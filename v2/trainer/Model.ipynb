{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.6193 - accuracy: 0.8419\n",
      "Epoch 1: val_loss improved from inf to 1.45755, saving model to isthemountainout.best.h5\n",
      "249/249 [==============================] - 113s 417ms/step - loss: 0.6193 - accuracy: 0.8419 - val_loss: 1.4576 - val_accuracy: 0.5089\n",
      "Epoch 2/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.6249 - accuracy: 0.8358\n",
      "Epoch 2: val_loss improved from 1.45755 to 0.55486, saving model to isthemountainout.best.h5\n",
      "249/249 [==============================] - 101s 404ms/step - loss: 0.6249 - accuracy: 0.8358 - val_loss: 0.5549 - val_accuracy: 0.8808\n",
      "Epoch 3/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.6195 - accuracy: 0.8314\n",
      "Epoch 3: val_loss did not improve from 0.55486\n",
      "249/249 [==============================] - 101s 402ms/step - loss: 0.6195 - accuracy: 0.8314 - val_loss: 1.0924 - val_accuracy: 0.5314\n",
      "Epoch 4/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.6185 - accuracy: 0.8286\n",
      "Epoch 4: val_loss improved from 0.55486 to 0.50953, saving model to isthemountainout.best.h5\n",
      "249/249 [==============================] - 101s 404ms/step - loss: 0.6185 - accuracy: 0.8286 - val_loss: 0.5095 - val_accuracy: 0.8969\n",
      "Epoch 5/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5959 - accuracy: 0.8423\n",
      "Epoch 5: val_loss improved from 0.50953 to 0.48795, saving model to isthemountainout.best.h5\n",
      "249/249 [==============================] - 101s 404ms/step - loss: 0.5959 - accuracy: 0.8423 - val_loss: 0.4880 - val_accuracy: 0.9163\n",
      "Epoch 6/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5971 - accuracy: 0.8423\n",
      "Epoch 6: val_loss did not improve from 0.48795\n",
      "249/249 [==============================] - 101s 403ms/step - loss: 0.5971 - accuracy: 0.8423 - val_loss: 0.6686 - val_accuracy: 0.7971\n",
      "Epoch 7/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5771 - accuracy: 0.8559\n",
      "Epoch 7: val_loss improved from 0.48795 to 0.47242, saving model to isthemountainout.best.h5\n",
      "249/249 [==============================] - 101s 404ms/step - loss: 0.5771 - accuracy: 0.8559 - val_loss: 0.4724 - val_accuracy: 0.9227\n",
      "Epoch 8/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5673 - accuracy: 0.8579\n",
      "Epoch 8: val_loss did not improve from 0.47242\n",
      "249/249 [==============================] - 101s 403ms/step - loss: 0.5673 - accuracy: 0.8579 - val_loss: 0.4764 - val_accuracy: 0.9082\n",
      "Epoch 9/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5610 - accuracy: 0.8575\n",
      "Epoch 9: val_loss improved from 0.47242 to 0.45423, saving model to isthemountainout.best.h5\n",
      "249/249 [==============================] - 101s 404ms/step - loss: 0.5610 - accuracy: 0.8575 - val_loss: 0.4542 - val_accuracy: 0.9163\n",
      "Epoch 10/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5560 - accuracy: 0.8612\n",
      "Epoch 10: val_loss improved from 0.45423 to 0.45261, saving model to isthemountainout.best.h5\n",
      "249/249 [==============================] - 101s 404ms/step - loss: 0.5560 - accuracy: 0.8612 - val_loss: 0.4526 - val_accuracy: 0.9227\n",
      "Epoch 11/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5992 - accuracy: 0.8406\n",
      "Epoch 11: val_loss did not improve from 0.45261\n",
      "249/249 [==============================] - 101s 403ms/step - loss: 0.5992 - accuracy: 0.8406 - val_loss: 0.6088 - val_accuracy: 0.8583\n",
      "Epoch 12/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5984 - accuracy: 0.8487\n",
      "Epoch 12: val_loss did not improve from 0.45261\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5984 - accuracy: 0.8487 - val_loss: 0.7905 - val_accuracy: 0.6812\n",
      "Epoch 13/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.6069 - accuracy: 0.8302\n",
      "Epoch 13: val_loss did not improve from 0.45261\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.6069 - accuracy: 0.8302 - val_loss: 0.5515 - val_accuracy: 0.8824\n",
      "Epoch 14/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5991 - accuracy: 0.8503\n",
      "Epoch 14: val_loss did not improve from 0.45261\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5991 - accuracy: 0.8503 - val_loss: 0.5202 - val_accuracy: 0.8841\n",
      "Epoch 15/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5949 - accuracy: 0.8495\n",
      "Epoch 15: val_loss did not improve from 0.45261\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5949 - accuracy: 0.8495 - val_loss: 2.0072 - val_accuracy: 0.5040\n",
      "Epoch 16/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5890 - accuracy: 0.8588\n",
      "Epoch 16: val_loss did not improve from 0.45261\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5890 - accuracy: 0.8588 - val_loss: 0.6360 - val_accuracy: 0.8502\n",
      "Epoch 17/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5759 - accuracy: 0.8559\n",
      "Epoch 17: val_loss did not improve from 0.45261\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5759 - accuracy: 0.8559 - val_loss: 0.5286 - val_accuracy: 0.8889\n",
      "Epoch 18/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5708 - accuracy: 0.8539\n",
      "Epoch 18: val_loss did not improve from 0.45261\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5708 - accuracy: 0.8539 - val_loss: 0.5011 - val_accuracy: 0.9018\n",
      "Epoch 19/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5794 - accuracy: 0.8575\n",
      "Epoch 19: val_loss did not improve from 0.45261\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5794 - accuracy: 0.8575 - val_loss: 0.6652 - val_accuracy: 0.8406\n",
      "Epoch 20/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5762 - accuracy: 0.8499\n",
      "Epoch 20: val_loss did not improve from 0.45261\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5762 - accuracy: 0.8499 - val_loss: 0.4696 - val_accuracy: 0.9114\n",
      "Epoch 21/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5618 - accuracy: 0.8636\n",
      "Epoch 21: val_loss did not improve from 0.45261\n",
      "249/249 [==============================] - 101s 402ms/step - loss: 0.5618 - accuracy: 0.8636 - val_loss: 0.4831 - val_accuracy: 0.9018\n",
      "Epoch 22/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5523 - accuracy: 0.8551\n",
      "Epoch 22: val_loss improved from 0.45261 to 0.43919, saving model to isthemountainout.best.h5\n",
      "249/249 [==============================] - 100s 402ms/step - loss: 0.5523 - accuracy: 0.8551 - val_loss: 0.4392 - val_accuracy: 0.9291\n",
      "Epoch 23/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5495 - accuracy: 0.8616\n",
      "Epoch 23: val_loss did not improve from 0.43919\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5495 - accuracy: 0.8616 - val_loss: 0.4659 - val_accuracy: 0.9179\n",
      "Epoch 24/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5393 - accuracy: 0.8724\n",
      "Epoch 24: val_loss did not improve from 0.43919\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5393 - accuracy: 0.8724 - val_loss: 0.4560 - val_accuracy: 0.9195\n",
      "Epoch 25/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5363 - accuracy: 0.8801\n",
      "Epoch 25: val_loss improved from 0.43919 to 0.43474, saving model to isthemountainout.best.h5\n",
      "249/249 [==============================] - 101s 402ms/step - loss: 0.5363 - accuracy: 0.8801 - val_loss: 0.4347 - val_accuracy: 0.9324\n",
      "Epoch 26/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5328 - accuracy: 0.8708\n",
      "Epoch 26: val_loss did not improve from 0.43474\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5328 - accuracy: 0.8708 - val_loss: 0.4353 - val_accuracy: 0.9291\n",
      "Epoch 27/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5360 - accuracy: 0.8700\n",
      "Epoch 27: val_loss did not improve from 0.43474\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5360 - accuracy: 0.8700 - val_loss: 0.4372 - val_accuracy: 0.9259\n",
      "Epoch 28/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5284 - accuracy: 0.8757\n",
      "Epoch 28: val_loss did not improve from 0.43474\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5284 - accuracy: 0.8757 - val_loss: 0.4394 - val_accuracy: 0.9259\n",
      "Epoch 29/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5092 - accuracy: 0.8893\n",
      "Epoch 29: val_loss improved from 0.43474 to 0.43449, saving model to isthemountainout.best.h5\n",
      "249/249 [==============================] - 100s 402ms/step - loss: 0.5092 - accuracy: 0.8893 - val_loss: 0.4345 - val_accuracy: 0.9291\n",
      "Epoch 30/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5354 - accuracy: 0.8700\n",
      "Epoch 30: val_loss did not improve from 0.43449\n",
      "249/249 [==============================] - 102s 408ms/step - loss: 0.5354 - accuracy: 0.8700 - val_loss: 0.4351 - val_accuracy: 0.9291\n",
      "Epoch 31/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5748 - accuracy: 0.8628\n",
      "Epoch 31: val_loss did not improve from 0.43449\n",
      "249/249 [==============================] - 101s 404ms/step - loss: 0.5748 - accuracy: 0.8628 - val_loss: 0.6697 - val_accuracy: 0.7858\n",
      "Epoch 32/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5762 - accuracy: 0.8571\n",
      "Epoch 32: val_loss did not improve from 0.43449\n",
      "249/249 [==============================] - 101s 403ms/step - loss: 0.5762 - accuracy: 0.8571 - val_loss: 0.5690 - val_accuracy: 0.8776\n",
      "Epoch 33/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5744 - accuracy: 0.8664\n",
      "Epoch 33: val_loss did not improve from 0.43449\n",
      "249/249 [==============================] - 101s 402ms/step - loss: 0.5744 - accuracy: 0.8664 - val_loss: 0.5913 - val_accuracy: 0.8760\n",
      "Epoch 34/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5562 - accuracy: 0.8716\n",
      "Epoch 34: val_loss did not improve from 0.43449\n",
      "249/249 [==============================] - 101s 403ms/step - loss: 0.5562 - accuracy: 0.8716 - val_loss: 0.5226 - val_accuracy: 0.9147\n",
      "Epoch 35/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5624 - accuracy: 0.8668\n",
      "Epoch 35: val_loss did not improve from 0.43449\n",
      "249/249 [==============================] - 101s 402ms/step - loss: 0.5624 - accuracy: 0.8668 - val_loss: 0.5153 - val_accuracy: 0.9163\n",
      "Epoch 36/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5637 - accuracy: 0.8660\n",
      "Epoch 36: val_loss did not improve from 0.43449\n",
      "249/249 [==============================] - 101s 402ms/step - loss: 0.5637 - accuracy: 0.8660 - val_loss: 0.4732 - val_accuracy: 0.9211\n",
      "Epoch 37/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5603 - accuracy: 0.8660\n",
      "Epoch 37: val_loss did not improve from 0.43449\n",
      "249/249 [==============================] - 101s 402ms/step - loss: 0.5603 - accuracy: 0.8660 - val_loss: 0.4754 - val_accuracy: 0.9147\n",
      "Epoch 38/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5521 - accuracy: 0.8688\n",
      "Epoch 38: val_loss did not improve from 0.43449\n",
      "249/249 [==============================] - 101s 403ms/step - loss: 0.5521 - accuracy: 0.8688 - val_loss: 0.6350 - val_accuracy: 0.8712\n",
      "Epoch 39/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5545 - accuracy: 0.8761\n",
      "Epoch 39: val_loss did not improve from 0.43449\n",
      "249/249 [==============================] - 101s 403ms/step - loss: 0.5545 - accuracy: 0.8761 - val_loss: 1.3689 - val_accuracy: 0.5137\n",
      "Epoch 40/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5457 - accuracy: 0.8684\n",
      "Epoch 40: val_loss did not improve from 0.43449\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5457 - accuracy: 0.8684 - val_loss: 0.4615 - val_accuracy: 0.9340\n",
      "Epoch 41/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5552 - accuracy: 0.8668\n",
      "Epoch 41: val_loss did not improve from 0.43449\n",
      "249/249 [==============================] - 100s 402ms/step - loss: 0.5552 - accuracy: 0.8668 - val_loss: 0.5508 - val_accuracy: 0.9259\n",
      "Epoch 42/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5392 - accuracy: 0.8757\n",
      "Epoch 42: val_loss improved from 0.43449 to 0.41288, saving model to isthemountainout.best.h5\n",
      "249/249 [==============================] - 100s 402ms/step - loss: 0.5392 - accuracy: 0.8757 - val_loss: 0.4129 - val_accuracy: 0.9404\n",
      "Epoch 43/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5386 - accuracy: 0.8781\n",
      "Epoch 43: val_loss did not improve from 0.41288\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5386 - accuracy: 0.8781 - val_loss: 0.4803 - val_accuracy: 0.9163\n",
      "Epoch 44/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5416 - accuracy: 0.8724\n",
      "Epoch 44: val_loss did not improve from 0.41288\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5416 - accuracy: 0.8724 - val_loss: 0.4322 - val_accuracy: 0.9372\n",
      "Epoch 45/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5252 - accuracy: 0.8825\n",
      "Epoch 45: val_loss did not improve from 0.41288\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5252 - accuracy: 0.8825 - val_loss: 0.4488 - val_accuracy: 0.9372\n",
      "Epoch 46/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5258 - accuracy: 0.8732\n",
      "Epoch 46: val_loss did not improve from 0.41288\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5258 - accuracy: 0.8732 - val_loss: 0.4555 - val_accuracy: 0.9372\n",
      "Epoch 47/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5311 - accuracy: 0.8688\n",
      "Epoch 47: val_loss did not improve from 0.41288\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5311 - accuracy: 0.8688 - val_loss: 0.4749 - val_accuracy: 0.9195\n",
      "Epoch 48/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5326 - accuracy: 0.8744\n",
      "Epoch 48: val_loss did not improve from 0.41288\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5326 - accuracy: 0.8744 - val_loss: 0.6559 - val_accuracy: 0.8164\n",
      "Epoch 49/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5316 - accuracy: 0.8773\n",
      "Epoch 49: val_loss improved from 0.41288 to 0.40598, saving model to isthemountainout.best.h5\n",
      "249/249 [==============================] - 101s 402ms/step - loss: 0.5316 - accuracy: 0.8773 - val_loss: 0.4060 - val_accuracy: 0.9404\n",
      "Epoch 50/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5189 - accuracy: 0.8869\n",
      "Epoch 50: val_loss did not improve from 0.40598\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5189 - accuracy: 0.8869 - val_loss: 0.4528 - val_accuracy: 0.9275\n",
      "Epoch 51/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5144 - accuracy: 0.8922\n",
      "Epoch 51: val_loss did not improve from 0.40598\n",
      "249/249 [==============================] - 101s 402ms/step - loss: 0.5144 - accuracy: 0.8922 - val_loss: 0.4138 - val_accuracy: 0.9404\n",
      "Epoch 52/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5167 - accuracy: 0.8849\n",
      "Epoch 52: val_loss did not improve from 0.40598\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5167 - accuracy: 0.8849 - val_loss: 0.4339 - val_accuracy: 0.9420\n",
      "Epoch 53/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5103 - accuracy: 0.8901\n",
      "Epoch 53: val_loss did not improve from 0.40598\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5103 - accuracy: 0.8901 - val_loss: 0.4152 - val_accuracy: 0.9469\n",
      "Epoch 54/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5256 - accuracy: 0.8748\n",
      "Epoch 54: val_loss did not improve from 0.40598\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5256 - accuracy: 0.8748 - val_loss: 0.4130 - val_accuracy: 0.9436\n",
      "Epoch 55/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5151 - accuracy: 0.8797\n",
      "Epoch 55: val_loss improved from 0.40598 to 0.39979, saving model to isthemountainout.best.h5\n",
      "249/249 [==============================] - 100s 402ms/step - loss: 0.5151 - accuracy: 0.8797 - val_loss: 0.3998 - val_accuracy: 0.9485\n",
      "Epoch 56/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5059 - accuracy: 0.8881\n",
      "Epoch 56: val_loss did not improve from 0.39979\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5059 - accuracy: 0.8881 - val_loss: 0.4006 - val_accuracy: 0.9501\n",
      "Epoch 57/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5209 - accuracy: 0.8797\n",
      "Epoch 57: val_loss did not improve from 0.39979\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5209 - accuracy: 0.8797 - val_loss: 0.4022 - val_accuracy: 0.9533\n",
      "Epoch 58/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5090 - accuracy: 0.8869\n",
      "Epoch 58: val_loss improved from 0.39979 to 0.38949, saving model to isthemountainout.best.h5\n",
      "249/249 [==============================] - 100s 402ms/step - loss: 0.5090 - accuracy: 0.8869 - val_loss: 0.3895 - val_accuracy: 0.9565\n",
      "Epoch 59/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5076 - accuracy: 0.8922\n",
      "Epoch 59: val_loss did not improve from 0.38949\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5076 - accuracy: 0.8922 - val_loss: 0.3956 - val_accuracy: 0.9485\n",
      "Epoch 60/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5120 - accuracy: 0.8833\n",
      "Epoch 60: val_loss improved from 0.38949 to 0.38600, saving model to isthemountainout.best.h5\n",
      "249/249 [==============================] - 101s 402ms/step - loss: 0.5120 - accuracy: 0.8833 - val_loss: 0.3860 - val_accuracy: 0.9469\n",
      "Epoch 61/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5050 - accuracy: 0.8905\n",
      "Epoch 61: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 101s 402ms/step - loss: 0.5050 - accuracy: 0.8905 - val_loss: 0.3906 - val_accuracy: 0.9501\n",
      "Epoch 62/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4975 - accuracy: 0.8930\n",
      "Epoch 62: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.4975 - accuracy: 0.8930 - val_loss: 0.3905 - val_accuracy: 0.9517\n",
      "Epoch 63/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4926 - accuracy: 0.8966\n",
      "Epoch 63: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.4926 - accuracy: 0.8966 - val_loss: 0.3878 - val_accuracy: 0.9501\n",
      "Epoch 64/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4982 - accuracy: 0.8897\n",
      "Epoch 64: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.4982 - accuracy: 0.8897 - val_loss: 0.3862 - val_accuracy: 0.9501\n",
      "Epoch 65/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5000 - accuracy: 0.8982\n",
      "Epoch 65: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5000 - accuracy: 0.8982 - val_loss: 0.3866 - val_accuracy: 0.9501\n",
      "Epoch 66/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4939 - accuracy: 0.8909\n",
      "Epoch 66: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.4939 - accuracy: 0.8909 - val_loss: 0.3908 - val_accuracy: 0.9485\n",
      "Epoch 67/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4891 - accuracy: 0.9014\n",
      "Epoch 67: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.4891 - accuracy: 0.9014 - val_loss: 0.3900 - val_accuracy: 0.9420\n",
      "Epoch 68/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4913 - accuracy: 0.8966\n",
      "Epoch 68: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.4913 - accuracy: 0.8966 - val_loss: 0.3900 - val_accuracy: 0.9420\n",
      "Epoch 69/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4803 - accuracy: 0.8998\n",
      "Epoch 69: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.4803 - accuracy: 0.8998 - val_loss: 0.3905 - val_accuracy: 0.9420\n",
      "Epoch 70/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4994 - accuracy: 0.8930\n",
      "Epoch 70: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.4994 - accuracy: 0.8930 - val_loss: 0.3906 - val_accuracy: 0.9436\n",
      "Epoch 71/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5348 - accuracy: 0.8781\n",
      "Epoch 71: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 101s 402ms/step - loss: 0.5348 - accuracy: 0.8781 - val_loss: 0.4901 - val_accuracy: 0.9275\n",
      "Epoch 72/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5338 - accuracy: 0.8712\n",
      "Epoch 72: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5338 - accuracy: 0.8712 - val_loss: 0.5263 - val_accuracy: 0.9388\n",
      "Epoch 73/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5364 - accuracy: 0.8720\n",
      "Epoch 73: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5364 - accuracy: 0.8720 - val_loss: 0.8503 - val_accuracy: 0.7536\n",
      "Epoch 74/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5398 - accuracy: 0.8648\n",
      "Epoch 74: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5398 - accuracy: 0.8648 - val_loss: 0.4784 - val_accuracy: 0.9195\n",
      "Epoch 75/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5337 - accuracy: 0.8813\n",
      "Epoch 75: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5337 - accuracy: 0.8813 - val_loss: 0.4348 - val_accuracy: 0.9388\n",
      "Epoch 76/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5408 - accuracy: 0.8765\n",
      "Epoch 76: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5408 - accuracy: 0.8765 - val_loss: 0.4580 - val_accuracy: 0.9243\n",
      "Epoch 77/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5285 - accuracy: 0.8753\n",
      "Epoch 77: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5285 - accuracy: 0.8753 - val_loss: 0.5071 - val_accuracy: 0.9372\n",
      "Epoch 78/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5313 - accuracy: 0.8704\n",
      "Epoch 78: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5313 - accuracy: 0.8704 - val_loss: 0.4196 - val_accuracy: 0.9436\n",
      "Epoch 79/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5311 - accuracy: 0.8789\n",
      "Epoch 79: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5311 - accuracy: 0.8789 - val_loss: 0.4393 - val_accuracy: 0.9452\n",
      "Epoch 80/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5323 - accuracy: 0.8813\n",
      "Epoch 80: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5323 - accuracy: 0.8813 - val_loss: 0.5493 - val_accuracy: 0.9130\n",
      "Epoch 81/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5330 - accuracy: 0.8748\n",
      "Epoch 81: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 101s 402ms/step - loss: 0.5330 - accuracy: 0.8748 - val_loss: 0.5585 - val_accuracy: 0.9098\n",
      "Epoch 82/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5200 - accuracy: 0.8861\n",
      "Epoch 82: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5200 - accuracy: 0.8861 - val_loss: 0.6495 - val_accuracy: 0.8293\n",
      "Epoch 83/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5234 - accuracy: 0.8712\n",
      "Epoch 83: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5234 - accuracy: 0.8712 - val_loss: 0.4324 - val_accuracy: 0.9404\n",
      "Epoch 84/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5403 - accuracy: 0.8684\n",
      "Epoch 84: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5403 - accuracy: 0.8684 - val_loss: 0.4396 - val_accuracy: 0.9340\n",
      "Epoch 85/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5214 - accuracy: 0.8821\n",
      "Epoch 85: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5214 - accuracy: 0.8821 - val_loss: 0.7735 - val_accuracy: 0.7279\n",
      "Epoch 86/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5310 - accuracy: 0.8781\n",
      "Epoch 86: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5310 - accuracy: 0.8781 - val_loss: 0.4153 - val_accuracy: 0.9452\n",
      "Epoch 87/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5169 - accuracy: 0.8773\n",
      "Epoch 87: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5169 - accuracy: 0.8773 - val_loss: 0.4744 - val_accuracy: 0.9324\n",
      "Epoch 88/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5299 - accuracy: 0.8736\n",
      "Epoch 88: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5299 - accuracy: 0.8736 - val_loss: 0.5370 - val_accuracy: 0.9130\n",
      "Epoch 89/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5280 - accuracy: 0.8825\n",
      "Epoch 89: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5280 - accuracy: 0.8825 - val_loss: 0.4156 - val_accuracy: 0.9501\n",
      "Epoch 90/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5269 - accuracy: 0.8753\n",
      "Epoch 90: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5269 - accuracy: 0.8753 - val_loss: 0.4771 - val_accuracy: 0.9308\n",
      "Epoch 91/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5201 - accuracy: 0.8825\n",
      "Epoch 91: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 101s 402ms/step - loss: 0.5201 - accuracy: 0.8825 - val_loss: 0.4032 - val_accuracy: 0.9420\n",
      "Epoch 92/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5157 - accuracy: 0.8861\n",
      "Epoch 92: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5157 - accuracy: 0.8861 - val_loss: 0.4212 - val_accuracy: 0.9388\n",
      "Epoch 93/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5130 - accuracy: 0.8897\n",
      "Epoch 93: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5130 - accuracy: 0.8897 - val_loss: 0.4530 - val_accuracy: 0.9356\n",
      "Epoch 94/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5103 - accuracy: 0.8885\n",
      "Epoch 94: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5103 - accuracy: 0.8885 - val_loss: 0.5114 - val_accuracy: 0.9340\n",
      "Epoch 95/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5122 - accuracy: 0.8825\n",
      "Epoch 95: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 101s 403ms/step - loss: 0.5122 - accuracy: 0.8825 - val_loss: 0.4086 - val_accuracy: 0.9452\n",
      "Epoch 96/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5101 - accuracy: 0.8861\n",
      "Epoch 96: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5101 - accuracy: 0.8861 - val_loss: 0.4108 - val_accuracy: 0.9420\n",
      "Epoch 97/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5084 - accuracy: 0.8901\n",
      "Epoch 97: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5084 - accuracy: 0.8901 - val_loss: 0.4027 - val_accuracy: 0.9517\n",
      "Epoch 98/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5059 - accuracy: 0.8821\n",
      "Epoch 98: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 402ms/step - loss: 0.5059 - accuracy: 0.8821 - val_loss: 0.4446 - val_accuracy: 0.9420\n",
      "Epoch 99/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5179 - accuracy: 0.8797\n",
      "Epoch 99: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5179 - accuracy: 0.8797 - val_loss: 0.4559 - val_accuracy: 0.9404\n",
      "Epoch 100/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5206 - accuracy: 0.8769\n",
      "Epoch 100: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.5206 - accuracy: 0.8769 - val_loss: 0.4098 - val_accuracy: 0.9501\n",
      "Epoch 101/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5108 - accuracy: 0.8881\n",
      "Epoch 101: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 402ms/step - loss: 0.5108 - accuracy: 0.8881 - val_loss: 0.4064 - val_accuracy: 0.9452\n",
      "Epoch 102/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5114 - accuracy: 0.8873\n",
      "Epoch 102: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.5114 - accuracy: 0.8873 - val_loss: 0.4444 - val_accuracy: 0.9420\n",
      "Epoch 103/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5042 - accuracy: 0.8857\n",
      "Epoch 103: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 106s 425ms/step - loss: 0.5042 - accuracy: 0.8857 - val_loss: 0.4060 - val_accuracy: 0.9436\n",
      "Epoch 104/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5045 - accuracy: 0.8889\n",
      "Epoch 104: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 106s 423ms/step - loss: 0.5045 - accuracy: 0.8889 - val_loss: 0.4257 - val_accuracy: 0.9291\n",
      "Epoch 105/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5066 - accuracy: 0.8905\n",
      "Epoch 105: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 106s 425ms/step - loss: 0.5066 - accuracy: 0.8905 - val_loss: 0.4298 - val_accuracy: 0.9420\n",
      "Epoch 106/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5041 - accuracy: 0.8805\n",
      "Epoch 106: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 108s 430ms/step - loss: 0.5041 - accuracy: 0.8805 - val_loss: 0.4004 - val_accuracy: 0.9469\n",
      "Epoch 107/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4999 - accuracy: 0.8950\n",
      "Epoch 107: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 106s 422ms/step - loss: 0.4999 - accuracy: 0.8950 - val_loss: 0.3943 - val_accuracy: 0.9420\n",
      "Epoch 108/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4976 - accuracy: 0.8970\n",
      "Epoch 108: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 103s 412ms/step - loss: 0.4976 - accuracy: 0.8970 - val_loss: 0.4460 - val_accuracy: 0.9372\n",
      "Epoch 109/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4971 - accuracy: 0.9014\n",
      "Epoch 109: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 103s 413ms/step - loss: 0.4971 - accuracy: 0.9014 - val_loss: 0.4274 - val_accuracy: 0.9469\n",
      "Epoch 110/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5078 - accuracy: 0.8861\n",
      "Epoch 110: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 103s 411ms/step - loss: 0.5078 - accuracy: 0.8861 - val_loss: 0.3975 - val_accuracy: 0.9420\n",
      "Epoch 111/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4890 - accuracy: 0.8938\n",
      "Epoch 111: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 104s 418ms/step - loss: 0.4890 - accuracy: 0.8938 - val_loss: 0.4353 - val_accuracy: 0.9308\n",
      "Epoch 112/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4823 - accuracy: 0.9010\n",
      "Epoch 112: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 109s 435ms/step - loss: 0.4823 - accuracy: 0.9010 - val_loss: 0.4180 - val_accuracy: 0.9340\n",
      "Epoch 113/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5008 - accuracy: 0.8913\n",
      "Epoch 113: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 106s 425ms/step - loss: 0.5008 - accuracy: 0.8913 - val_loss: 0.4744 - val_accuracy: 0.9452\n",
      "Epoch 114/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4895 - accuracy: 0.8994\n",
      "Epoch 114: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 103s 412ms/step - loss: 0.4895 - accuracy: 0.8994 - val_loss: 0.4121 - val_accuracy: 0.9436\n",
      "Epoch 115/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4839 - accuracy: 0.8942\n",
      "Epoch 115: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 103s 412ms/step - loss: 0.4839 - accuracy: 0.8942 - val_loss: 0.3983 - val_accuracy: 0.9501\n",
      "Epoch 116/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4902 - accuracy: 0.8938\n",
      "Epoch 116: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 103s 412ms/step - loss: 0.4902 - accuracy: 0.8938 - val_loss: 0.3873 - val_accuracy: 0.9533\n",
      "Epoch 117/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4813 - accuracy: 0.9030\n",
      "Epoch 117: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 104s 415ms/step - loss: 0.4813 - accuracy: 0.9030 - val_loss: 0.3942 - val_accuracy: 0.9452\n",
      "Epoch 118/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4888 - accuracy: 0.8978\n",
      "Epoch 118: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 103s 412ms/step - loss: 0.4888 - accuracy: 0.8978 - val_loss: 0.4211 - val_accuracy: 0.9469\n",
      "Epoch 119/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4885 - accuracy: 0.8922\n",
      "Epoch 119: val_loss did not improve from 0.38600\n",
      "249/249 [==============================] - 103s 412ms/step - loss: 0.4885 - accuracy: 0.8922 - val_loss: 0.4007 - val_accuracy: 0.9485\n",
      "Epoch 120/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4821 - accuracy: 0.9066\n",
      "Epoch 120: val_loss improved from 0.38600 to 0.38300, saving model to isthemountainout.best.h5\n",
      "249/249 [==============================] - 104s 414ms/step - loss: 0.4821 - accuracy: 0.9066 - val_loss: 0.3830 - val_accuracy: 0.9485\n",
      "Epoch 121/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4797 - accuracy: 0.9014\n",
      "Epoch 121: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 104s 414ms/step - loss: 0.4797 - accuracy: 0.9014 - val_loss: 0.3899 - val_accuracy: 0.9485\n",
      "Epoch 122/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4860 - accuracy: 0.9066\n",
      "Epoch 122: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 103s 412ms/step - loss: 0.4860 - accuracy: 0.9066 - val_loss: 0.3917 - val_accuracy: 0.9485\n",
      "Epoch 123/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4846 - accuracy: 0.8974\n",
      "Epoch 123: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 102s 408ms/step - loss: 0.4846 - accuracy: 0.8974 - val_loss: 0.4032 - val_accuracy: 0.9549\n",
      "Epoch 124/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4847 - accuracy: 0.8974\n",
      "Epoch 124: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.4847 - accuracy: 0.8974 - val_loss: 0.3884 - val_accuracy: 0.9533\n",
      "Epoch 125/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4825 - accuracy: 0.9022\n",
      "Epoch 125: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.4825 - accuracy: 0.9022 - val_loss: 0.3957 - val_accuracy: 0.9517\n",
      "Epoch 126/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4790 - accuracy: 0.9054\n",
      "Epoch 126: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.4790 - accuracy: 0.9054 - val_loss: 0.3886 - val_accuracy: 0.9517\n",
      "Epoch 127/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4797 - accuracy: 0.9082\n",
      "Epoch 127: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.4797 - accuracy: 0.9082 - val_loss: 0.3944 - val_accuracy: 0.9485\n",
      "Epoch 128/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4726 - accuracy: 0.9070\n",
      "Epoch 128: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.4726 - accuracy: 0.9070 - val_loss: 0.3859 - val_accuracy: 0.9517\n",
      "Epoch 129/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4666 - accuracy: 0.9042\n",
      "Epoch 129: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.4666 - accuracy: 0.9042 - val_loss: 0.3897 - val_accuracy: 0.9469\n",
      "Epoch 130/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4791 - accuracy: 0.9026\n",
      "Epoch 130: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 100s 402ms/step - loss: 0.4791 - accuracy: 0.9026 - val_loss: 0.3903 - val_accuracy: 0.9469\n",
      "Epoch 131/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4781 - accuracy: 0.9014\n",
      "Epoch 131: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 101s 402ms/step - loss: 0.4781 - accuracy: 0.9014 - val_loss: 0.3904 - val_accuracy: 0.9501\n",
      "Epoch 132/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4715 - accuracy: 0.9091\n",
      "Epoch 132: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.4715 - accuracy: 0.9091 - val_loss: 0.3876 - val_accuracy: 0.9517\n",
      "Epoch 133/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4658 - accuracy: 0.9107\n",
      "Epoch 133: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.4658 - accuracy: 0.9107 - val_loss: 0.3899 - val_accuracy: 0.9549\n",
      "Epoch 134/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4691 - accuracy: 0.9074\n",
      "Epoch 134: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.4691 - accuracy: 0.9074 - val_loss: 0.3929 - val_accuracy: 0.9549\n",
      "Epoch 135/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4665 - accuracy: 0.9010\n",
      "Epoch 135: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.4665 - accuracy: 0.9010 - val_loss: 0.3944 - val_accuracy: 0.9517\n",
      "Epoch 136/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4752 - accuracy: 0.9046\n",
      "Epoch 136: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.4752 - accuracy: 0.9046 - val_loss: 0.3947 - val_accuracy: 0.9517\n",
      "Epoch 137/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4664 - accuracy: 0.9074\n",
      "Epoch 137: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 100s 400ms/step - loss: 0.4664 - accuracy: 0.9074 - val_loss: 0.3914 - val_accuracy: 0.9517\n",
      "Epoch 138/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4695 - accuracy: 0.9050\n",
      "Epoch 138: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.4695 - accuracy: 0.9050 - val_loss: 0.3920 - val_accuracy: 0.9485\n",
      "Epoch 139/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4745 - accuracy: 0.9070\n",
      "Epoch 139: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.4745 - accuracy: 0.9070 - val_loss: 0.3945 - val_accuracy: 0.9549\n",
      "Epoch 140/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4658 - accuracy: 0.9123\n",
      "Epoch 140: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 100s 401ms/step - loss: 0.4658 - accuracy: 0.9123 - val_loss: 0.3954 - val_accuracy: 0.9517\n",
      "Epoch 141/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4742 - accuracy: 0.9014\n",
      "Epoch 141: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 101s 404ms/step - loss: 0.4742 - accuracy: 0.9014 - val_loss: 0.3931 - val_accuracy: 0.9517\n",
      "Epoch 142/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4645 - accuracy: 0.9038\n",
      "Epoch 142: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 101s 402ms/step - loss: 0.4645 - accuracy: 0.9038 - val_loss: 0.3970 - val_accuracy: 0.9517\n",
      "Epoch 143/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4721 - accuracy: 0.9095\n",
      "Epoch 143: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 100s 402ms/step - loss: 0.4721 - accuracy: 0.9095 - val_loss: 0.3971 - val_accuracy: 0.9485\n",
      "Epoch 144/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4695 - accuracy: 0.9058\n",
      "Epoch 144: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 100s 402ms/step - loss: 0.4695 - accuracy: 0.9058 - val_loss: 0.3962 - val_accuracy: 0.9501\n",
      "Epoch 145/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4752 - accuracy: 0.8978\n",
      "Epoch 145: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 101s 402ms/step - loss: 0.4752 - accuracy: 0.8978 - val_loss: 0.3958 - val_accuracy: 0.9517\n",
      "Epoch 146/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4649 - accuracy: 0.9139\n",
      "Epoch 146: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 103s 411ms/step - loss: 0.4649 - accuracy: 0.9139 - val_loss: 0.3955 - val_accuracy: 0.9485\n",
      "Epoch 147/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4690 - accuracy: 0.8986\n",
      "Epoch 147: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 104s 414ms/step - loss: 0.4690 - accuracy: 0.8986 - val_loss: 0.3956 - val_accuracy: 0.9501\n",
      "Epoch 148/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4680 - accuracy: 0.9034\n",
      "Epoch 148: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 104s 418ms/step - loss: 0.4680 - accuracy: 0.9034 - val_loss: 0.3953 - val_accuracy: 0.9501\n",
      "Epoch 149/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4563 - accuracy: 0.9070\n",
      "Epoch 149: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 103s 414ms/step - loss: 0.4563 - accuracy: 0.9070 - val_loss: 0.3958 - val_accuracy: 0.9485\n",
      "Epoch 150/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4695 - accuracy: 0.9103\n",
      "Epoch 150: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 104s 416ms/step - loss: 0.4695 - accuracy: 0.9103 - val_loss: 0.3952 - val_accuracy: 0.9501\n",
      "Epoch 151/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5140 - accuracy: 0.8881\n",
      "Epoch 151: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 104s 414ms/step - loss: 0.5140 - accuracy: 0.8881 - val_loss: 0.4237 - val_accuracy: 0.9420\n",
      "Epoch 152/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5030 - accuracy: 0.8926\n",
      "Epoch 152: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 106s 424ms/step - loss: 0.5030 - accuracy: 0.8926 - val_loss: 0.4380 - val_accuracy: 0.9372\n",
      "Epoch 153/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5136 - accuracy: 0.8893\n",
      "Epoch 153: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 105s 418ms/step - loss: 0.5136 - accuracy: 0.8893 - val_loss: 0.4897 - val_accuracy: 0.9356\n",
      "Epoch 154/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5172 - accuracy: 0.8809\n",
      "Epoch 154: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 105s 419ms/step - loss: 0.5172 - accuracy: 0.8809 - val_loss: 0.5658 - val_accuracy: 0.8921\n",
      "Epoch 155/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5103 - accuracy: 0.8881\n",
      "Epoch 155: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 106s 424ms/step - loss: 0.5103 - accuracy: 0.8881 - val_loss: 0.7429 - val_accuracy: 0.7230\n",
      "Epoch 156/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5059 - accuracy: 0.8845\n",
      "Epoch 156: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 108s 431ms/step - loss: 0.5059 - accuracy: 0.8845 - val_loss: 0.4766 - val_accuracy: 0.9243\n",
      "Epoch 157/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5103 - accuracy: 0.8865\n",
      "Epoch 157: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 109s 436ms/step - loss: 0.5103 - accuracy: 0.8865 - val_loss: 0.3983 - val_accuracy: 0.9501\n",
      "Epoch 158/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5009 - accuracy: 0.8881\n",
      "Epoch 158: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 107s 428ms/step - loss: 0.5009 - accuracy: 0.8881 - val_loss: 0.4717 - val_accuracy: 0.9372\n",
      "Epoch 159/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4981 - accuracy: 0.8885\n",
      "Epoch 159: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 106s 425ms/step - loss: 0.4981 - accuracy: 0.8885 - val_loss: 0.5458 - val_accuracy: 0.8969\n",
      "Epoch 160/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5047 - accuracy: 0.8861\n",
      "Epoch 160: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 104s 418ms/step - loss: 0.5047 - accuracy: 0.8861 - val_loss: 0.5386 - val_accuracy: 0.9259\n",
      "Epoch 161/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5078 - accuracy: 0.8889\n",
      "Epoch 161: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 105s 420ms/step - loss: 0.5078 - accuracy: 0.8889 - val_loss: 0.4615 - val_accuracy: 0.9356\n",
      "Epoch 162/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4991 - accuracy: 0.8877\n",
      "Epoch 162: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 106s 424ms/step - loss: 0.4991 - accuracy: 0.8877 - val_loss: 0.4792 - val_accuracy: 0.9179\n",
      "Epoch 163/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5034 - accuracy: 0.8913\n",
      "Epoch 163: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 105s 422ms/step - loss: 0.5034 - accuracy: 0.8913 - val_loss: 0.4557 - val_accuracy: 0.9436\n",
      "Epoch 164/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5035 - accuracy: 0.8877\n",
      "Epoch 164: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 106s 423ms/step - loss: 0.5035 - accuracy: 0.8877 - val_loss: 0.4063 - val_accuracy: 0.9501\n",
      "Epoch 165/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5007 - accuracy: 0.8909\n",
      "Epoch 165: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 107s 430ms/step - loss: 0.5007 - accuracy: 0.8909 - val_loss: 0.4366 - val_accuracy: 0.9436\n",
      "Epoch 166/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5002 - accuracy: 0.8893\n",
      "Epoch 166: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 109s 437ms/step - loss: 0.5002 - accuracy: 0.8893 - val_loss: 0.4242 - val_accuracy: 0.9469\n",
      "Epoch 167/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5096 - accuracy: 0.8857\n",
      "Epoch 167: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 107s 430ms/step - loss: 0.5096 - accuracy: 0.8857 - val_loss: 0.4405 - val_accuracy: 0.9485\n",
      "Epoch 168/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5020 - accuracy: 0.8897\n",
      "Epoch 168: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 111s 442ms/step - loss: 0.5020 - accuracy: 0.8897 - val_loss: 0.4269 - val_accuracy: 0.9469\n",
      "Epoch 169/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4995 - accuracy: 0.8946\n",
      "Epoch 169: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 110s 438ms/step - loss: 0.4995 - accuracy: 0.8946 - val_loss: 0.4689 - val_accuracy: 0.9340\n",
      "Epoch 170/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5024 - accuracy: 0.8918\n",
      "Epoch 170: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 110s 441ms/step - loss: 0.5024 - accuracy: 0.8918 - val_loss: 0.4027 - val_accuracy: 0.9549\n",
      "Epoch 171/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5086 - accuracy: 0.8825\n",
      "Epoch 171: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 111s 446ms/step - loss: 0.5086 - accuracy: 0.8825 - val_loss: 0.4036 - val_accuracy: 0.9436\n",
      "Epoch 172/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.5010 - accuracy: 0.8930\n",
      "Epoch 172: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 111s 444ms/step - loss: 0.5010 - accuracy: 0.8930 - val_loss: 0.4254 - val_accuracy: 0.9420\n",
      "Epoch 173/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4942 - accuracy: 0.8986\n",
      "Epoch 173: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 108s 433ms/step - loss: 0.4942 - accuracy: 0.8986 - val_loss: 0.4603 - val_accuracy: 0.9436\n",
      "Epoch 174/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4969 - accuracy: 0.8930\n",
      "Epoch 174: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 106s 425ms/step - loss: 0.4969 - accuracy: 0.8930 - val_loss: 0.4137 - val_accuracy: 0.9420\n",
      "Epoch 175/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4918 - accuracy: 0.9018\n",
      "Epoch 175: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 107s 428ms/step - loss: 0.4918 - accuracy: 0.9018 - val_loss: 0.4552 - val_accuracy: 0.9404\n",
      "Epoch 176/700\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.4971 - accuracy: 0.8942\n",
      "Epoch 176: val_loss did not improve from 0.38300\n",
      "249/249 [==============================] - 108s 432ms/step - loss: 0.4971 - accuracy: 0.8942 - val_loss: 0.4131 - val_accuracy: 0.9404\n",
      "Epoch 177/700\n",
      "123/249 [=============>................] - ETA: 51s - loss: 0.4973 - accuracy: 0.8902"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'IteratorGetNext' defined at (most recent call last):\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Taylor\\AppData\\Local\\Temp\\ipykernel_24196\\2578742606.py\", line 23, in <module>\n      model.fit(\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\keras\\engine\\training.py\", line 1145, in step_function\n      data = next(iterator)\nNode: 'IteratorGetNext'\nDetected at node 'IteratorGetNext' defined at (most recent call last):\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Taylor\\AppData\\Local\\Temp\\ipykernel_24196\\2578742606.py\", line 23, in <module>\n      model.fit(\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\keras\\engine\\training.py\", line 1145, in step_function\n      data = next(iterator)\nNode: 'IteratorGetNext'\n2 root error(s) found.\n  (0) RESOURCE_EXHAUSTED:  Failed to allocate memory for the batch of component 0\n\t [[{{node IteratorGetNext}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[model/random_brightness/loop_body/stateful_uniform/Bitcast/pfor/while/body/_378/model/random_brightness/loop_body/stateful_uniform/Bitcast/pfor/while/TensorArrayV2Write/TensorListSetItem/_244]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n  (1) RESOURCE_EXHAUSTED:  Failed to allocate memory for the batch of component 0\n\t [[{{node IteratorGetNext}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_16536]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 23\u001b[0m\n\u001b[0;32m     10\u001b[0m model \u001b[39m=\u001b[39m frozenmodel\u001b[39m.\u001b[39mgenerate_model(\n\u001b[0;32m     11\u001b[0m     weights_filepath\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39misthemountainout.best.h5\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     12\u001b[0m     with_augmentations\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     13\u001b[0m     can_ignore_weights\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m model\u001b[39m.\u001b[39mcompile(\n\u001b[0;32m     15\u001b[0m     optimizer\u001b[39m=\u001b[39moptimizers\u001b[39m.\u001b[39mAdadelta(\n\u001b[0;32m     16\u001b[0m         learning_rate\u001b[39m=\u001b[39moptimizers\u001b[39m.\u001b[39mschedules\u001b[39m.\u001b[39mCosineDecayRestarts(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     22\u001b[0m )\n\u001b[1;32m---> 23\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     24\u001b[0m     dataset\u001b[39m.\u001b[39;49mtraining,\n\u001b[0;32m     25\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m700\u001b[39;49m,\n\u001b[0;32m     26\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     27\u001b[0m     validation_data\u001b[39m=\u001b[39;49mdataset\u001b[39m.\u001b[39;49mvalidation,\n\u001b[0;32m     28\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[\n\u001b[0;32m     29\u001b[0m         callbacks\u001b[39m.\u001b[39;49mTensorBoard(\n\u001b[0;32m     30\u001b[0m             log_dir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlogs/fit/\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m datetime\u001b[39m.\u001b[39;49mnow()\u001b[39m.\u001b[39;49mstrftime(\u001b[39m\"\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mY\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mm\u001b[39;49m\u001b[39m%d\u001b[39;49;00m\u001b[39m-\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mH\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mM\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mS\u001b[39;49m\u001b[39m\"\u001b[39;49m)),\n\u001b[0;32m     31\u001b[0m         tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mcallbacks\u001b[39m.\u001b[39;49mModelCheckpoint(\n\u001b[0;32m     32\u001b[0m             \u001b[39m'\u001b[39;49m\u001b[39misthemountainout.best.h5\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     33\u001b[0m             monitor\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mval_loss\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     34\u001b[0m             mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmin\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     35\u001b[0m             save_best_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     36\u001b[0m             verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m),\n\u001b[0;32m     37\u001b[0m         tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mcallbacks\u001b[39m.\u001b[39;49mEarlyStopping(\n\u001b[0;32m     38\u001b[0m             monitor\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mval_loss\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     39\u001b[0m             mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmin\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     40\u001b[0m             patience\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[0;32m     41\u001b[0m             restore_best_weights\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     42\u001b[0m             verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m),\n\u001b[0;32m     43\u001b[0m         tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mcallbacks\u001b[39m.\u001b[39;49mTensorBoard(\n\u001b[0;32m     44\u001b[0m             log_dir\u001b[39m=\u001b[39;49mos\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(\u001b[39m'\u001b[39;49m\u001b[39mtensor-logs\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mfit\u001b[39;49m\u001b[39m'\u001b[39;49m, datetime\u001b[39m.\u001b[39;49mnow()\u001b[39m.\u001b[39;49mstrftime(\u001b[39m'\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mY\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mm\u001b[39;49m\u001b[39m%d\u001b[39;49;00m\u001b[39m%\u001b[39;49m\u001b[39mH\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mM\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mS\u001b[39;49m\u001b[39m'\u001b[39;49m)),\n\u001b[0;32m     45\u001b[0m             update_freq\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[0;32m     46\u001b[0m             write_images\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     47\u001b[0m             write_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     48\u001b[0m             embeddings_freq\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m),\n\u001b[0;32m     49\u001b[0m     ])\n",
      "File \u001b[1;32mc:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'IteratorGetNext' defined at (most recent call last):\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Taylor\\AppData\\Local\\Temp\\ipykernel_24196\\2578742606.py\", line 23, in <module>\n      model.fit(\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\keras\\engine\\training.py\", line 1145, in step_function\n      data = next(iterator)\nNode: 'IteratorGetNext'\nDetected at node 'IteratorGetNext' defined at (most recent call last):\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Taylor\\AppData\\Local\\Temp\\ipykernel_24196\\2578742606.py\", line 23, in <module>\n      model.fit(\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\Taylor\\anaconda3\\envs\\mountain\\lib\\site-packages\\keras\\engine\\training.py\", line 1145, in step_function\n      data = next(iterator)\nNode: 'IteratorGetNext'\n2 root error(s) found.\n  (0) RESOURCE_EXHAUSTED:  Failed to allocate memory for the batch of component 0\n\t [[{{node IteratorGetNext}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[model/random_brightness/loop_body/stateful_uniform/Bitcast/pfor/while/body/_378/model/random_brightness/loop_body/stateful_uniform/Bitcast/pfor/while/TensorArrayV2Write/TensorListSetItem/_244]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n  (1) RESOURCE_EXHAUSTED:  Failed to allocate memory for the batch of component 0\n\t [[{{node IteratorGetNext}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_16536]"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "%run setup/GpuOptions.ipynb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "'..' not in sys.path and sys.path.append('..')\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import importlib\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from common import frozenmodel\n",
    "from common.dataset import Dataset\n",
    "from tensorflow.keras import callbacks, optimizers, losses\n",
    "importlib.reload(frozenmodel)\n",
    "\n",
    "dataset = Dataset(\n",
    "    '../dataset/dataset-2022-10-19T22-51-08',\n",
    "    batch_size=10,\n",
    "    image_size=(1080, 1920))\n",
    "\n",
    "model = frozenmodel.generate_model(\n",
    "    weights_filepath='isthemountainout.best.h5',\n",
    "    with_augmentations=True,\n",
    "    can_ignore_weights=True)\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adadelta(\n",
    "        learning_rate=optimizers.schedules.CosineDecayRestarts(\n",
    "            2.5,\n",
    "            # Decay over 5 epochs and then restart \n",
    "            dataset.training.cardinality().numpy() * 10)),\n",
    "    loss=losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.08),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "model.fit(\n",
    "    dataset.training,\n",
    "    epochs=700,\n",
    "    verbose=True,\n",
    "    validation_data=dataset.validation,\n",
    "    callbacks=[\n",
    "        callbacks.TensorBoard(\n",
    "            log_dir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            'isthemountainout.best.h5',\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_best_only=True,\n",
    "            verbose=True),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            patience=100,\n",
    "            restore_best_weights=True,\n",
    "            verbose=True),\n",
    "        tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=os.path.join('tensor-logs', 'fit', datetime.now().strftime('%Y%m%d%H%M%S')),\n",
    "            update_freq=50,\n",
    "            write_images=True,\n",
    "            write_graph=True,\n",
    "            embeddings_freq=10),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n",
      "Enabling memory growth on PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "loading model with best weights\n",
      "Found 3106 files belonging to 4 classes.\n",
      "Using 2175 files for training.\n",
      "Found 3106 files belonging to 4 classes.\n",
      "Using 931 files for validation.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run setup/GpuOptions.ipynb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "'..' not in sys.path and sys.path.append('..')\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import importlib\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from common import frozenmodel\n",
    "from common.dataset import Dataset\n",
    "importlib.reload(frozenmodel)\n",
    "\n",
    "print('loading model with best weights')\n",
    "# from common.weights import weights\n",
    "# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '../isthemountainout-credentials.json'\n",
    "# with weights() as filename:\n",
    "model = frozenmodel.generate_model(\n",
    "    weights_filepath='isthemountainout.h5', # filename\n",
    "    with_augmentations=True)\n",
    "\n",
    "dataset = Dataset(\n",
    "    '../dataset/dataset-2022-10-19T22-51-08',\n",
    "    batch_size=6,\n",
    "    image_size=(1080, 1920),\n",
    "    validation_split=0.3)\n",
    "validations = dataset.validation\n",
    "predictions = []\n",
    "labels = []\n",
    "for data, label in validations:\n",
    "  labels.append(tf.argmax(label, axis=1))\n",
    "  predictions.append(tf.math.argmax(tf.nn.softmax(model.predict(data, verbose=0)), axis=1))\n",
    "labels = tf.concat(labels, axis=0)\n",
    "predictions = tf.concat(predictions, axis=0)\n",
    "cm = confusion_matrix(labels, predictions, labels=list(range(len(dataset.validation.class_names))))\n",
    "cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "sns.heatmap(\n",
    "  cmn,\n",
    "  annot=True,\n",
    "  fmt='.2f',\n",
    "  xticklabels=dataset.validation.class_names,\n",
    "  yticklabels=dataset.validation.class_names)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show(block=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mountain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "675b856ca8a14374dc4009cf40e9ead9d95917e285fd3b96229cb66bb660ef16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
