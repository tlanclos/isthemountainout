{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2987 images belonging to 4 classes.\n",
      "Found 745 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "%run GpuOptions.ipynb\n",
    "%run BuildTrainingData.ipynb\n",
    "%run TrendTools.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 112, 112, 32) 896         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 112, 112, 32) 128         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 112, 112, 32) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 112, 112, 64) 18496       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 112, 112, 64) 256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 112, 112, 64) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 112, 112, 64) 0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_2 (SeparableCo (None, 112, 112, 128 8896        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 112, 112, 128 512         separable_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 112, 112, 128 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_3 (SeparableCo (None, 112, 112, 128 17664       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 112, 112, 128 512         separable_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 56, 56, 128)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 56, 56, 128)  8320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 56, 56, 128)  0           max_pooling2d[0][0]              \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 56, 56, 128)  0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_4 (SeparableCo (None, 56, 56, 256)  34176       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 56, 56, 256)  1024        separable_conv2d_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 56, 56, 256)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_5 (SeparableCo (None, 56, 56, 256)  68096       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 56, 56, 256)  1024        separable_conv2d_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 28, 28, 256)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 28, 28, 256)  33024       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 28, 28, 256)  0           max_pooling2d_1[0][0]            \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 28, 28, 256)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_6 (SeparableCo (None, 28, 28, 728)  189400      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 28, 28, 728)  2912        separable_conv2d_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 28, 28, 728)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_7 (SeparableCo (None, 28, 28, 728)  537264      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 28, 28, 728)  2912        separable_conv2d_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 14, 14, 728)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 14, 14, 728)  187096      add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 14, 14, 728)  0           max_pooling2d_2[0][0]            \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 14, 14, 728)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_8 (SeparableCo (None, 14, 14, 728)  537264      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 14, 14, 728)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_9 (SeparableCo (None, 14, 14, 728)  537264      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 14, 14, 728)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_10 (SeparableC (None, 14, 14, 728)  537264      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 14, 14, 728)  0           batch_normalization_12[0][0]     \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 14, 14, 728)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_11 (SeparableC (None, 14, 14, 728)  537264      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 14, 14, 728)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_12 (SeparableC (None, 14, 14, 728)  537264      activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 14, 14, 728)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_13 (SeparableC (None, 14, 14, 728)  537264      activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 14, 14, 728)  0           batch_normalization_15[0][0]     \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 14, 14, 728)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_14 (SeparableC (None, 14, 14, 728)  537264      activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 14, 14, 728)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_15 (SeparableC (None, 14, 14, 728)  537264      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_15[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 14, 14, 728)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_16 (SeparableC (None, 14, 14, 728)  537264      activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_16[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 14, 14, 728)  0           batch_normalization_18[0][0]     \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 14, 14, 728)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_17 (SeparableC (None, 14, 14, 728)  537264      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_17[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 14, 14, 728)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_18 (SeparableC (None, 14, 14, 728)  537264      activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_18[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 14, 14, 728)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_19 (SeparableC (None, 14, 14, 728)  537264      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_19[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 14, 14, 728)  0           batch_normalization_21[0][0]     \n",
      "                                                                 add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 14, 14, 728)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_20 (SeparableC (None, 14, 14, 728)  537264      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_20[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 14, 14, 728)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_21 (SeparableC (None, 14, 14, 728)  537264      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_21[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 14, 14, 728)  0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_22 (SeparableC (None, 14, 14, 728)  537264      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_22[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 14, 14, 728)  0           batch_normalization_24[0][0]     \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 14, 14, 728)  0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_23 (SeparableC (None, 14, 14, 728)  537264      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_23[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 14, 14, 728)  0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_24 (SeparableC (None, 14, 14, 728)  537264      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_24[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 14, 14, 728)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_25 (SeparableC (None, 14, 14, 728)  537264      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_25[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 14, 14, 728)  0           batch_normalization_27[0][0]     \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 14, 14, 728)  0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_26 (SeparableC (None, 14, 14, 728)  537264      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_26[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 14, 14, 728)  0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_27 (SeparableC (None, 14, 14, 728)  537264      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_27[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 14, 14, 728)  0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_28 (SeparableC (None, 14, 14, 728)  537264      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_28[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 14, 14, 728)  0           batch_normalization_30[0][0]     \n",
      "                                                                 add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 14, 14, 728)  0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_29 (SeparableC (None, 14, 14, 728)  537264      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_29[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 14, 14, 728)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_30 (SeparableC (None, 14, 14, 728)  537264      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_30[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 14, 14, 728)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_31 (SeparableC (None, 14, 14, 728)  537264      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_31[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 14, 14, 728)  0           batch_normalization_33[0][0]     \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 14, 14, 728)  0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_32 (SeparableC (None, 14, 14, 728)  537264      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_32[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 14, 14, 728)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_33 (SeparableC (None, 14, 14, 1024) 753048      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 14, 14, 1024) 4096        separable_conv2d_33[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 7, 7, 1024)   0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 7, 7, 1024)   746496      add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 7, 7, 1024)   0           max_pooling2d_3[0][0]            \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 7, 7, 1024)   0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d (SeparableConv (None, 7, 7, 728)    755416      activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 7, 7, 728)    2912        separable_conv2d[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 7, 7, 728)    0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_1 (SeparableCo (None, 7, 7, 1024)   753048      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 7, 7, 1024)   4096        separable_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 1024)         0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 4)            4100        global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 4)            0           dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 17,640,220\n",
      "Trainable params: 17,593,628\n",
      "Non-trainable params: 46,592\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import os\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow.keras.layers import *\r\n",
    "import common.model as m\r\n",
    "\r\n",
    "classes = training_data_generator.class_indices\r\n",
    "data, _ = training_data_generator.next()\r\n",
    "shape = data[0].shape\r\n",
    "inputs = tf.keras.Input(shape=shape)\r\n",
    "\r\n",
    "outputs = m.chained(\r\n",
    "    # Entry Flow\r\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, padding='same'),\r\n",
    "    tf.keras.layers.BatchNormalization(),\r\n",
    "    tf.keras.layers.Activation('relu'),\r\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same'),\r\n",
    "    tf.keras.layers.BatchNormalization(),\r\n",
    "    tf.keras.layers.Activation('relu'),\r\n",
    "    m.expand(\r\n",
    "        flow=lambda previous_activation, size: tf.keras.layers.add([\r\n",
    "            m.chained(\r\n",
    "                m.duplicate(\r\n",
    "                    layers=lambda: [\r\n",
    "                        tf.keras.layers.Activation('relu'),\r\n",
    "                        tf.keras.layers.SeparableConv2D(filters=size, kernel_size=3, padding='same'),\r\n",
    "                        tf.keras.layers.BatchNormalization(),\r\n",
    "                    ],\r\n",
    "                    count=2\r\n",
    "                ),\r\n",
    "                tf.keras.layers.MaxPooling2D(pool_size=3, strides=2, padding='same'),\r\n",
    "            )(previous_activation),\r\n",
    "            tf.keras.layers.Conv2D(filters=size, kernel_size=1, strides=2, padding='same')(previous_activation),\r\n",
    "        ]),\r\n",
    "        values=[128, 256, 728],\r\n",
    "    ),\r\n",
    "\r\n",
    "    # # Middle Flow\r\n",
    "    m.expand(\r\n",
    "        flow=lambda previous_activation, _: tf.keras.layers.add([\r\n",
    "            m.duplicate(\r\n",
    "                layers=lambda: [\r\n",
    "                    tf.keras.layers.Activation('relu'),\r\n",
    "                    tf.keras.layers.SeparableConv2D(filters=728, kernel_size=3, padding='same'),\r\n",
    "                    tf.keras.layers.BatchNormalization(),\r\n",
    "                ],\r\n",
    "                count=3,\r\n",
    "            )(previous_activation),\r\n",
    "            previous_activation,\r\n",
    "        ]),\r\n",
    "        values=[0] * 8\r\n",
    "    ),\r\n",
    "\r\n",
    "    # # Exit Flow\r\n",
    "    lambda previous_activation: tf.keras.layers.add([\r\n",
    "        m.chained(\r\n",
    "            tf.keras.layers.Activation('relu'),\r\n",
    "            tf.keras.layers.SeparableConv2D(filters=728, kernel_size=3, padding='same'),\r\n",
    "            tf.keras.layers.BatchNormalization(),\r\n",
    "            tf.keras.layers.Activation('relu'),\r\n",
    "            tf.keras.layers.SeparableConv2D(filters=1024, kernel_size=3, padding='same'),\r\n",
    "            tf.keras.layers.BatchNormalization(),\r\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=3, strides=2, padding='same'),\r\n",
    "        )(previous_activation),\r\n",
    "        tf.keras.layers.Conv2D(filters=1024, kernel_size=1, strides=2, padding='same')(previous_activation),\r\n",
    "    ]),\r\n",
    "    tf.keras.layers.Activation('relu'),\r\n",
    "    tf.keras.layers.SeparableConv2D(filters=728, kernel_size=3, padding='same'),\r\n",
    "    tf.keras.layers.BatchNormalization(),\r\n",
    "    tf.keras.layers.Activation('relu'),\r\n",
    "    tf.keras.layers.SeparableConv2D(filters=1024, kernel_size=3, padding='same'),\r\n",
    "    tf.keras.layers.BatchNormalization(),\r\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\r\n",
    "    tf.keras.layers.Dense(len(classes), activation='linear'),\r\n",
    "    tf.keras.layers.Dropout(0.2),\r\n",
    ")(inputs)\r\n",
    "\r\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n",
    "model.build(input_shape=(None, *shape))\r\n",
    "model.compile(\r\n",
    "    optimizer=tf.keras.optimizers.Adadelta(learning_rate=0.008),\r\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(\r\n",
    "        from_logits=True, label_smoothing=0.1),\r\n",
    "    metrics=['accuracy'],\r\n",
    ")\r\n",
    "if os.path.exists('isthemountainout.h5'):\r\n",
    "    model.load_weights('isthemountainout.h5')\r\n",
    "elif os.path.exists('isthemountainout.best.h5'):\r\n",
    "    model.load_weights('isthemountainout.best.h5')\r\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3732 images belonging to 4 classes.\n",
      "Epoch 1/700\n",
      "  1/746 [..............................] - ETA: 0s - loss: 0.7215 - accuracy: 0.7500WARNING:tensorflow:From C:\\Users\\Taylor\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "  2/746 [..............................] - ETA: 1:13 - loss: 0.7640 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0390s vs `on_train_batch_end` time: 0.1578s). Check your callbacks.\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5437 - accuracy: 0.9018\n",
      "Epoch 00001: val_loss improved from inf to 0.58902, saving model to isthemountainout.best.h5\n",
      "746/746 [==============================] - 112s 151ms/step - loss: 0.5437 - accuracy: 0.9018 - val_loss: 0.5890 - val_accuracy: 0.8898\n",
      "Epoch 2/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5378 - accuracy: 0.9092\n",
      "Epoch 00002: val_loss improved from 0.58902 to 0.57511, saving model to isthemountainout.best.h5\n",
      "746/746 [==============================] - 95s 128ms/step - loss: 0.5378 - accuracy: 0.9092 - val_loss: 0.5751 - val_accuracy: 0.8898\n",
      "Epoch 3/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5414 - accuracy: 0.9041\n",
      "Epoch 00003: val_loss did not improve from 0.57511\n",
      "746/746 [==============================] - 107s 144ms/step - loss: 0.5414 - accuracy: 0.9041 - val_loss: 0.5975 - val_accuracy: 0.8790\n",
      "Epoch 4/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5509 - accuracy: 0.8971\n",
      "Epoch 00004: val_loss improved from 0.57511 to 0.56339, saving model to isthemountainout.best.h5\n",
      "746/746 [==============================] - 94s 126ms/step - loss: 0.5509 - accuracy: 0.8971 - val_loss: 0.5634 - val_accuracy: 0.8938\n",
      "Epoch 5/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5466 - accuracy: 0.8998\n",
      "Epoch 00005: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 106s 143ms/step - loss: 0.5466 - accuracy: 0.8998 - val_loss: 0.6115 - val_accuracy: 0.8817\n",
      "Epoch 6/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5324 - accuracy: 0.9115\n",
      "Epoch 00006: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 93s 125ms/step - loss: 0.5324 - accuracy: 0.9115 - val_loss: 0.6098 - val_accuracy: 0.8831\n",
      "Epoch 7/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5569 - accuracy: 0.8978\n",
      "Epoch 00007: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 107s 143ms/step - loss: 0.5569 - accuracy: 0.8978 - val_loss: 0.5846 - val_accuracy: 0.8804\n",
      "Epoch 8/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5485 - accuracy: 0.8984\n",
      "Epoch 00008: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 93s 125ms/step - loss: 0.5485 - accuracy: 0.8984 - val_loss: 0.6005 - val_accuracy: 0.8790\n",
      "Epoch 9/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5392 - accuracy: 0.9008\n",
      "Epoch 00009: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 107s 143ms/step - loss: 0.5392 - accuracy: 0.9008 - val_loss: 0.6105 - val_accuracy: 0.8790\n",
      "Epoch 10/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5396 - accuracy: 0.9085\n",
      "Epoch 00010: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 93s 125ms/step - loss: 0.5396 - accuracy: 0.9085 - val_loss: 0.6038 - val_accuracy: 0.8844\n",
      "Epoch 11/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5269 - accuracy: 0.9065\n",
      "Epoch 00011: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 107s 144ms/step - loss: 0.5269 - accuracy: 0.9065 - val_loss: 0.5750 - val_accuracy: 0.8965\n",
      "Epoch 12/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5440 - accuracy: 0.9021\n",
      "Epoch 00012: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 94s 125ms/step - loss: 0.5440 - accuracy: 0.9021 - val_loss: 0.6008 - val_accuracy: 0.8817\n",
      "Epoch 13/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5419 - accuracy: 0.8981\n",
      "Epoch 00013: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 106s 143ms/step - loss: 0.5419 - accuracy: 0.8981 - val_loss: 0.6246 - val_accuracy: 0.8669\n",
      "Epoch 14/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5232 - accuracy: 0.9138\n",
      "Epoch 00014: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 94s 125ms/step - loss: 0.5232 - accuracy: 0.9138 - val_loss: 0.6249 - val_accuracy: 0.8669\n",
      "Epoch 15/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5243 - accuracy: 0.9105\n",
      "Epoch 00015: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 107s 143ms/step - loss: 0.5243 - accuracy: 0.9105 - val_loss: 0.6090 - val_accuracy: 0.8790\n",
      "Epoch 16/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5309 - accuracy: 0.9024\n",
      "Epoch 00016: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 93s 125ms/step - loss: 0.5309 - accuracy: 0.9024 - val_loss: 0.6020 - val_accuracy: 0.8790\n",
      "Epoch 17/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5421 - accuracy: 0.8984\n",
      "Epoch 00017: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 107s 143ms/step - loss: 0.5421 - accuracy: 0.8984 - val_loss: 0.6248 - val_accuracy: 0.8763\n",
      "Epoch 18/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5378 - accuracy: 0.9085\n",
      "Epoch 00018: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 93s 125ms/step - loss: 0.5378 - accuracy: 0.9085 - val_loss: 0.6409 - val_accuracy: 0.8562\n",
      "Epoch 19/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5308 - accuracy: 0.9125\n",
      "Epoch 00019: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 106s 143ms/step - loss: 0.5308 - accuracy: 0.9125 - val_loss: 0.6124 - val_accuracy: 0.8737\n",
      "Epoch 20/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5332 - accuracy: 0.9021\n",
      "Epoch 00020: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 94s 125ms/step - loss: 0.5332 - accuracy: 0.9021 - val_loss: 0.6627 - val_accuracy: 0.8535\n",
      "Epoch 21/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5338 - accuracy: 0.9078\n",
      "Epoch 00021: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 107s 143ms/step - loss: 0.5338 - accuracy: 0.9078 - val_loss: 0.6490 - val_accuracy: 0.8656\n",
      "Epoch 22/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5258 - accuracy: 0.9192\n",
      "Epoch 00022: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 94s 125ms/step - loss: 0.5258 - accuracy: 0.9192 - val_loss: 0.6080 - val_accuracy: 0.8723\n",
      "Epoch 23/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5282 - accuracy: 0.9085\n",
      "Epoch 00023: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 107s 143ms/step - loss: 0.5282 - accuracy: 0.9085 - val_loss: 0.6798 - val_accuracy: 0.8522\n",
      "Epoch 24/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5226 - accuracy: 0.9152\n",
      "Epoch 00024: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 93s 125ms/step - loss: 0.5226 - accuracy: 0.9152 - val_loss: 0.6534 - val_accuracy: 0.8629\n",
      "Epoch 25/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5325 - accuracy: 0.9081\n",
      "Epoch 00025: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 106s 143ms/step - loss: 0.5325 - accuracy: 0.9081 - val_loss: 0.5959 - val_accuracy: 0.8737\n",
      "Epoch 26/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5270 - accuracy: 0.9159\n",
      "Epoch 00026: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 94s 126ms/step - loss: 0.5270 - accuracy: 0.9159 - val_loss: 0.5919 - val_accuracy: 0.8898\n",
      "Epoch 27/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5231 - accuracy: 0.9152\n",
      "Epoch 00027: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 107s 143ms/step - loss: 0.5231 - accuracy: 0.9152 - val_loss: 0.5995 - val_accuracy: 0.8844\n",
      "Epoch 28/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5166 - accuracy: 0.9175\n",
      "Epoch 00028: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 93s 125ms/step - loss: 0.5166 - accuracy: 0.9175 - val_loss: 0.6340 - val_accuracy: 0.8737\n",
      "Epoch 29/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5198 - accuracy: 0.9175\n",
      "Epoch 00029: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 106s 143ms/step - loss: 0.5198 - accuracy: 0.9175 - val_loss: 0.6199 - val_accuracy: 0.8858\n",
      "Epoch 30/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5191 - accuracy: 0.9189\n",
      "Epoch 00030: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 93s 125ms/step - loss: 0.5191 - accuracy: 0.9189 - val_loss: 0.6285 - val_accuracy: 0.8750\n",
      "Epoch 31/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5214 - accuracy: 0.9098\n",
      "Epoch 00031: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 107s 144ms/step - loss: 0.5214 - accuracy: 0.9098 - val_loss: 0.6284 - val_accuracy: 0.8763\n",
      "Epoch 32/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5247 - accuracy: 0.9048\n",
      "Epoch 00032: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 94s 126ms/step - loss: 0.5247 - accuracy: 0.9048 - val_loss: 0.6036 - val_accuracy: 0.8844\n",
      "Epoch 33/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5304 - accuracy: 0.9065\n",
      "Epoch 00033: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 106s 142ms/step - loss: 0.5304 - accuracy: 0.9065 - val_loss: 0.6196 - val_accuracy: 0.8925\n",
      "Epoch 34/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5267 - accuracy: 0.9088\n",
      "Epoch 00034: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 93s 125ms/step - loss: 0.5267 - accuracy: 0.9088 - val_loss: 0.6252 - val_accuracy: 0.8831\n",
      "Epoch 35/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5439 - accuracy: 0.9105\n",
      "Epoch 00035: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 106s 142ms/step - loss: 0.5439 - accuracy: 0.9105 - val_loss: 0.6055 - val_accuracy: 0.8884\n",
      "Epoch 36/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5305 - accuracy: 0.9045\n",
      "Epoch 00036: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 93s 125ms/step - loss: 0.5305 - accuracy: 0.9045 - val_loss: 0.6097 - val_accuracy: 0.8817\n",
      "Epoch 37/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5255 - accuracy: 0.9031\n",
      "Epoch 00037: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 106s 142ms/step - loss: 0.5255 - accuracy: 0.9031 - val_loss: 0.6099 - val_accuracy: 0.8831\n",
      "Epoch 38/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5170 - accuracy: 0.9162\n",
      "Epoch 00038: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 93s 125ms/step - loss: 0.5170 - accuracy: 0.9162 - val_loss: 0.6187 - val_accuracy: 0.8790\n",
      "Epoch 39/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5187 - accuracy: 0.9172\n",
      "Epoch 00039: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 106s 143ms/step - loss: 0.5187 - accuracy: 0.9172 - val_loss: 0.6203 - val_accuracy: 0.8844\n",
      "Epoch 40/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5254 - accuracy: 0.9165\n",
      "Epoch 00040: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 93s 125ms/step - loss: 0.5254 - accuracy: 0.9165 - val_loss: 0.6313 - val_accuracy: 0.8777\n",
      "Epoch 41/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5295 - accuracy: 0.9031\n",
      "Epoch 00041: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 108s 144ms/step - loss: 0.5295 - accuracy: 0.9031 - val_loss: 0.6208 - val_accuracy: 0.8884\n",
      "Epoch 42/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5123 - accuracy: 0.9145\n",
      "Epoch 00042: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 94s 125ms/step - loss: 0.5123 - accuracy: 0.9145 - val_loss: 0.5947 - val_accuracy: 0.8898\n",
      "Epoch 43/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5234 - accuracy: 0.9192\n",
      "Epoch 00043: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 106s 142ms/step - loss: 0.5234 - accuracy: 0.9192 - val_loss: 0.6062 - val_accuracy: 0.8898\n",
      "Epoch 44/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5232 - accuracy: 0.9132\n",
      "Epoch 00044: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 93s 125ms/step - loss: 0.5232 - accuracy: 0.9132 - val_loss: 0.6116 - val_accuracy: 0.8831\n",
      "Epoch 45/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5297 - accuracy: 0.9045\n",
      "Epoch 00045: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 106s 143ms/step - loss: 0.5297 - accuracy: 0.9045 - val_loss: 0.6172 - val_accuracy: 0.8831\n",
      "Epoch 46/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5291 - accuracy: 0.9048\n",
      "Epoch 00046: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 93s 124ms/step - loss: 0.5291 - accuracy: 0.9048 - val_loss: 0.6208 - val_accuracy: 0.8804\n",
      "Epoch 47/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5210 - accuracy: 0.9068\n",
      "Epoch 00047: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 106s 142ms/step - loss: 0.5210 - accuracy: 0.9068 - val_loss: 0.6361 - val_accuracy: 0.8696\n",
      "Epoch 48/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5210 - accuracy: 0.9145\n",
      "Epoch 00048: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 93s 125ms/step - loss: 0.5210 - accuracy: 0.9145 - val_loss: 0.6386 - val_accuracy: 0.8777\n",
      "Epoch 49/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5293 - accuracy: 0.9028\n",
      "Epoch 00049: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 107s 143ms/step - loss: 0.5293 - accuracy: 0.9028 - val_loss: 0.6278 - val_accuracy: 0.8750\n",
      "Epoch 50/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5259 - accuracy: 0.9145\n",
      "Epoch 00050: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 93s 125ms/step - loss: 0.5259 - accuracy: 0.9145 - val_loss: 0.5907 - val_accuracy: 0.8925\n",
      "Epoch 51/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5153 - accuracy: 0.9159\n",
      "Epoch 00051: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 107s 144ms/step - loss: 0.5153 - accuracy: 0.9159 - val_loss: 0.6186 - val_accuracy: 0.8737\n",
      "Epoch 52/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5226 - accuracy: 0.9169\n",
      "Epoch 00052: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 94s 126ms/step - loss: 0.5226 - accuracy: 0.9169 - val_loss: 0.6124 - val_accuracy: 0.8898\n",
      "Epoch 53/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5228 - accuracy: 0.9061\n",
      "Epoch 00053: val_loss did not improve from 0.56339\n",
      "746/746 [==============================] - 107s 143ms/step - loss: 0.5228 - accuracy: 0.9061 - val_loss: 0.6377 - val_accuracy: 0.8790\n",
      "Epoch 54/700\n",
      "746/746 [==============================] - ETA: 0s - loss: 0.5127 - accuracy: 0.9219\n",
      "Epoch 00054: val_loss did not improve from 0.56339\n",
      "Restoring model weights from the end of the best epoch.\n",
      "746/746 [==============================] - 94s 126ms/step - loss: 0.5127 - accuracy: 0.9219 - val_loss: 0.6278 - val_accuracy: 0.8858\n",
      "Epoch 00054: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x208fc6b5cd0>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\r\n",
    "\r\n",
    "model.fit(\r\n",
    "    training_data_generator,\r\n",
    "    epochs=700,\r\n",
    "    verbose=True,\r\n",
    "    steps_per_epoch=training_data_generator.samples // \r\n",
    "        training_data_generator.batch_size,\r\n",
    "    validation_data=validation_data_generator,\r\n",
    "    validation_steps=validation_data_generator.samples // \r\n",
    "        validation_data_generator.batch_size,\r\n",
    "    callbacks=[\r\n",
    "        tf.keras.callbacks.ModelCheckpoint(\r\n",
    "            'isthemountainout.best.h5',\r\n",
    "            monitor='val_loss',\r\n",
    "            mode='min',\r\n",
    "            save_best_only=True,\r\n",
    "            verbose=True),\r\n",
    "        tf.keras.callbacks.EarlyStopping(\r\n",
    "            monitor='val_loss',\r\n",
    "            mode='min',\r\n",
    "            patience=50,\r\n",
    "            restore_best_weights=True,\r\n",
    "            verbose=True),\r\n",
    "        # tf.keras.callbacks.CSVLogger(os.path.join('logs', 'isthemountainout.training.csv')),\r\n",
    "        tf.keras.callbacks.TensorBoard(\r\n",
    "            log_dir=os.path.join('logs', 'fit', datetime.now().strftime('%Y%m%d%H%M%S')),\r\n",
    "            update_freq=50,\r\n",
    "            write_images=True,\r\n",
    "            write_graph=True,\r\n",
    "            embeddings_freq=10),\r\n",
    "        m.LogConfusionMatrixCallback(\r\n",
    "            model=model,\r\n",
    "            datagen=tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0 / 255)\\\r\n",
    "                .flow_from_directory(data_directory, batch_size=3096, shuffle=True, target_size=image_size),\r\n",
    "            logdir=os.path.join('logs', 'image'))\r\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('tf_gpu': conda)",
   "metadata": {
    "interpreter": {
     "hash": "611695c0bd76c6e875083607f81ce0a6f5bca983f3620a36074696f3391eb980"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}