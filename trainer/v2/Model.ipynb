{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found 2929 images belonging to 4 classes.\nFound 731 images belonging to 4 classes.\n"
    }
   ],
   "source": [
    "%run GpuOptions.ipynb\n",
    "%run BuildTrainingData.ipynb\n",
    "%run TrendTools.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"functional_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n__________________________________________________________________________________________________\nlambda (Lambda)                 (None, 224, 224, 1)  0           input_1[0][0]                    \n__________________________________________________________________________________________________\nlambda_1 (Lambda)               (None, 224, 224, 1)  0           input_1[0][0]                    \n__________________________________________________________________________________________________\nconv2d (Conv2D)                 (None, 222, 222, 64) 640         lambda[0][0]                     \n__________________________________________________________________________________________________\nconv2d_transpose (Conv2DTranspo (None, 226, 226, 64) 640         lambda_1[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d (MaxPooling2D)    (None, 111, 111, 64) 0           conv2d[0][0]                     \n__________________________________________________________________________________________________\nmax_pooling2d_3 (MaxPooling2D)  (None, 113, 113, 64) 0           conv2d_transpose[0][0]           \n__________________________________________________________________________________________________\nconv2d_1 (Conv2D)               (None, 109, 109, 64) 36928       max_pooling2d[0][0]              \n__________________________________________________________________________________________________\nconv2d_transpose_1 (Conv2DTrans (None, 115, 115, 64) 36928       max_pooling2d_3[0][0]            \n__________________________________________________________________________________________________\nmax_pooling2d_1 (MaxPooling2D)  (None, 54, 54, 64)   0           conv2d_1[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_4 (MaxPooling2D)  (None, 57, 57, 64)   0           conv2d_transpose_1[0][0]         \n__________________________________________________________________________________________________\nconv2d_2 (Conv2D)               (None, 52, 52, 64)   36928       max_pooling2d_1[0][0]            \n__________________________________________________________________________________________________\nconv2d_transpose_2 (Conv2DTrans (None, 59, 59, 64)   36928       max_pooling2d_4[0][0]            \n__________________________________________________________________________________________________\nmax_pooling2d_2 (MaxPooling2D)  (None, 26, 26, 64)   0           conv2d_2[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_5 (MaxPooling2D)  (None, 29, 29, 64)   0           conv2d_transpose_2[0][0]         \n__________________________________________________________________________________________________\nflatten (Flatten)               (None, 43264)        0           max_pooling2d_2[0][0]            \n__________________________________________________________________________________________________\nflatten_1 (Flatten)             (None, 53824)        0           max_pooling2d_5[0][0]            \n__________________________________________________________________________________________________\ndropout (Dropout)               (None, 43264)        0           flatten[0][0]                    \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 53824)        0           flatten_1[0][0]                  \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 32)           1384480     dropout[0][0]                    \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 32)           1722400     dropout_1[0][0]                  \n__________________________________________________________________________________________________\nadd (Add)                       (None, 32)           0           dense[0][0]                      \n                                                                 dense_1[0][0]                    \n__________________________________________________________________________________________________\ndropout_2 (Dropout)             (None, 32)           0           add[0][0]                        \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 4)            132         dropout_2[0][0]                  \n__________________________________________________________________________________________________\nactivation (Activation)         (None, 4)            0           dense_2[0][0]                    \n==================================================================================================\nTotal params: 3,256,004\nTrainable params: 3,256,004\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import common.model as m\n",
    "\n",
    "classes = training_data_generator.class_indices\n",
    "data, _ = training_data_generator.next()\n",
    "shape = data[0].shape\n",
    "inputs = tf.keras.Input(shape=shape)\n",
    "outputs = m.chained(\n",
    "    tf.keras.layers.add([\n",
    "        m.chained(\n",
    "            inputs,\n",
    "            tf.keras.layers.Lambda(\n",
    "                lambda image: tf.image.rgb_to_grayscale(image)),\n",
    "            tf.keras.layers.Conv2D(\n",
    "                filters=64, kernel_size=3, activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D(2),\n",
    "            tf.keras.layers.Conv2D(\n",
    "                filters=64, kernel_size=3, activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D(2),\n",
    "            tf.keras.layers.Conv2D(\n",
    "                filters=64, kernel_size=3, activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D(2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(32),\n",
    "        ),\n",
    "        m.chained(\n",
    "            inputs,\n",
    "            tf.keras.layers.Lambda(\n",
    "                lambda image: tf.image.rgb_to_grayscale(image)),\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                filters=64, kernel_size=3, activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D(2),\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                filters=64, kernel_size=3, activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D(2),\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                filters=64, kernel_size=3, activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D(2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(32),\n",
    "        ),\n",
    "    ]),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(len(classes)),\n",
    "    tf.keras.layers.Activation('softmax'))\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.build(input_shape=(None, *shape))\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adadelta(learning_rate=0.01),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(\n",
    "        from_logits=True, label_smoothing=0.05),\n",
    "    metrics=['accuracy'])\n",
    "if os.path.exists('isthemountainout.h5'):\n",
    "    model.load_weights('isthemountainout.h5')\n",
    "elif os.path.exists('isthemountainout.best.h5'):\n",
    "    model.load_weights('isthemountainout.best.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "loss: 0.8831 - val_accuracy: 0.8920\nEpoch 349/700\n91/91 [==============================] - ETA: 0s - loss: 0.8266 - accuracy: 0.9541\nEpoch 00349: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 55s 607ms/step - loss: 0.8266 - accuracy: 0.9541 - val_loss: 0.8856 - val_accuracy: 0.8906\nEpoch 350/700\n91/91 [==============================] - ETA: 0s - loss: 0.8266 - accuracy: 0.9562\nEpoch 00350: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 43s 473ms/step - loss: 0.8266 - accuracy: 0.9562 - val_loss: 0.8820 - val_accuracy: 0.8906\nEpoch 351/700\n91/91 [==============================] - ETA: 0s - loss: 0.8299 - accuracy: 0.9513\nEpoch 00351: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 55s 608ms/step - loss: 0.8299 - accuracy: 0.9513 - val_loss: 0.8807 - val_accuracy: 0.9006\nEpoch 352/700\n91/91 [==============================] - ETA: 0s - loss: 0.8293 - accuracy: 0.9513\nEpoch 00352: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 43s 473ms/step - loss: 0.8293 - accuracy: 0.9513 - val_loss: 0.8772 - val_accuracy: 0.9006\nEpoch 353/700\n91/91 [==============================] - ETA: 0s - loss: 0.8276 - accuracy: 0.9541\nEpoch 00353: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 55s 607ms/step - loss: 0.8276 - accuracy: 0.9541 - val_loss: 0.8806 - val_accuracy: 0.8935\nEpoch 354/700\n91/91 [==============================] - ETA: 0s - loss: 0.8288 - accuracy: 0.9513\nEpoch 00354: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 43s 475ms/step - loss: 0.8288 - accuracy: 0.9513 - val_loss: 0.8821 - val_accuracy: 0.8963\nEpoch 355/700\n91/91 [==============================] - ETA: 0s - loss: 0.8273 - accuracy: 0.9541\nEpoch 00355: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 56s 610ms/step - loss: 0.8273 - accuracy: 0.9541 - val_loss: 0.8827 - val_accuracy: 0.8906\nEpoch 356/700\n91/91 [==============================] - ETA: 0s - loss: 0.8256 - accuracy: 0.9565\nEpoch 00356: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 43s 475ms/step - loss: 0.8256 - accuracy: 0.9565 - val_loss: 0.8812 - val_accuracy: 0.8991\nEpoch 357/700\n91/91 [==============================] - ETA: 0s - loss: 0.8297 - accuracy: 0.9524\nEpoch 00357: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 56s 614ms/step - loss: 0.8297 - accuracy: 0.9524 - val_loss: 0.8820 - val_accuracy: 0.8963\nEpoch 358/700\n91/91 [==============================] - ETA: 0s - loss: 0.8289 - accuracy: 0.9524\nEpoch 00358: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 43s 475ms/step - loss: 0.8289 - accuracy: 0.9524 - val_loss: 0.8806 - val_accuracy: 0.8977\nEpoch 359/700\n91/91 [==============================] - ETA: 0s - loss: 0.8270 - accuracy: 0.9547\nEpoch 00359: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 55s 608ms/step - loss: 0.8270 - accuracy: 0.9547 - val_loss: 0.8806 - val_accuracy: 0.8935\nEpoch 360/700\n91/91 [==============================] - ETA: 0s - loss: 0.8303 - accuracy: 0.9499\nEpoch 00360: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 43s 474ms/step - loss: 0.8303 - accuracy: 0.9499 - val_loss: 0.8788 - val_accuracy: 0.8920\nEpoch 361/700\n91/91 [==============================] - ETA: 0s - loss: 0.8281 - accuracy: 0.9524\nEpoch 00361: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 56s 614ms/step - loss: 0.8281 - accuracy: 0.9524 - val_loss: 0.8840 - val_accuracy: 0.8920\nEpoch 362/700\n91/91 [==============================] - ETA: 0s - loss: 0.8283 - accuracy: 0.9537\nEpoch 00362: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 43s 472ms/step - loss: 0.8283 - accuracy: 0.9537 - val_loss: 0.8804 - val_accuracy: 0.8977\nEpoch 363/700\n91/91 [==============================] - ETA: 0s - loss: 0.8296 - accuracy: 0.9524\nEpoch 00363: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 55s 608ms/step - loss: 0.8296 - accuracy: 0.9524 - val_loss: 0.8863 - val_accuracy: 0.8864\nEpoch 364/700\n91/91 [==============================] - ETA: 0s - loss: 0.8266 - accuracy: 0.9544\nEpoch 00364: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 43s 475ms/step - loss: 0.8266 - accuracy: 0.9544 - val_loss: 0.8795 - val_accuracy: 0.8963\nEpoch 365/700\n91/91 [==============================] - ETA: 0s - loss: 0.8270 - accuracy: 0.9558\nEpoch 00365: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 55s 609ms/step - loss: 0.8270 - accuracy: 0.9558 - val_loss: 0.8787 - val_accuracy: 0.8963\nEpoch 366/700\n91/91 [==============================] - ETA: 0s - loss: 0.8291 - accuracy: 0.9524\nEpoch 00366: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 43s 477ms/step - loss: 0.8291 - accuracy: 0.9524 - val_loss: 0.8820 - val_accuracy: 0.8935\nEpoch 367/700\n91/91 [==============================] - ETA: 0s - loss: 0.8280 - accuracy: 0.9541\nEpoch 00367: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 55s 606ms/step - loss: 0.8280 - accuracy: 0.9541 - val_loss: 0.8803 - val_accuracy: 0.8963\nEpoch 368/700\n91/91 [==============================] - ETA: 0s - loss: 0.8295 - accuracy: 0.9496\nEpoch 00368: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 43s 476ms/step - loss: 0.8295 - accuracy: 0.9496 - val_loss: 0.8770 - val_accuracy: 0.8991\nEpoch 369/700\n91/91 [==============================] - ETA: 0s - loss: 0.8270 - accuracy: 0.9544\nEpoch 00369: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 55s 609ms/step - loss: 0.8270 - accuracy: 0.9544 - val_loss: 0.8794 - val_accuracy: 0.8963\nEpoch 370/700\n91/91 [==============================] - ETA: 0s - loss: 0.8263 - accuracy: 0.9555\nEpoch 00370: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 43s 476ms/step - loss: 0.8263 - accuracy: 0.9555 - val_loss: 0.8868 - val_accuracy: 0.8935\nEpoch 371/700\n91/91 [==============================] - ETA: 0s - loss: 0.8288 - accuracy: 0.9517\nEpoch 00371: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 55s 607ms/step - loss: 0.8288 - accuracy: 0.9517 - val_loss: 0.8798 - val_accuracy: 0.8963\nEpoch 372/700\n91/91 [==============================] - ETA: 0s - loss: 0.8266 - accuracy: 0.9537\nEpoch 00372: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 43s 475ms/step - loss: 0.8266 - accuracy: 0.9537 - val_loss: 0.8845 - val_accuracy: 0.8920\nEpoch 373/700\n91/91 [==============================] - ETA: 0s - loss: 0.8283 - accuracy: 0.9534\nEpoch 00373: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 55s 606ms/step - loss: 0.8283 - accuracy: 0.9534 - val_loss: 0.8822 - val_accuracy: 0.8892\nEpoch 374/700\n91/91 [==============================] - ETA: 0s - loss: 0.8266 - accuracy: 0.9555\nEpoch 00374: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 43s 476ms/step - loss: 0.8266 - accuracy: 0.9555 - val_loss: 0.8852 - val_accuracy: 0.8920\nEpoch 375/700\n91/91 [==============================] - ETA: 0s - loss: 0.8296 - accuracy: 0.9510\nEpoch 00375: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 55s 608ms/step - loss: 0.8296 - accuracy: 0.9510 - val_loss: 0.8792 - val_accuracy: 0.8949\nEpoch 376/700\n91/91 [==============================] - ETA: 0s - loss: 0.8253 - accuracy: 0.9575\nEpoch 00376: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 43s 474ms/step - loss: 0.8253 - accuracy: 0.9575 - val_loss: 0.8812 - val_accuracy: 0.8920\nEpoch 377/700\n91/91 [==============================] - ETA: 0s - loss: 0.8286 - accuracy: 0.9541\nEpoch 00377: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 56s 613ms/step - loss: 0.8286 - accuracy: 0.9541 - val_loss: 0.8784 - val_accuracy: 0.8977\nEpoch 378/700\n91/91 [==============================] - ETA: 0s - loss: 0.8281 - accuracy: 0.9534\nEpoch 00378: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 43s 474ms/step - loss: 0.8281 - accuracy: 0.9534 - val_loss: 0.8837 - val_accuracy: 0.8920\nEpoch 379/700\n91/91 [==============================] - ETA: 0s - loss: 0.8260 - accuracy: 0.9551\nEpoch 00379: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 55s 606ms/step - loss: 0.8260 - accuracy: 0.9551 - val_loss: 0.8811 - val_accuracy: 0.8935\nEpoch 380/700\n91/91 [==============================] - ETA: 0s - loss: 0.8281 - accuracy: 0.9534\nEpoch 00380: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 43s 476ms/step - loss: 0.8281 - accuracy: 0.9534 - val_loss: 0.8867 - val_accuracy: 0.8892\nEpoch 381/700\n91/91 [==============================] - ETA: 0s - loss: 0.8270 - accuracy: 0.9534\nEpoch 00381: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 55s 608ms/step - loss: 0.8270 - accuracy: 0.9534 - val_loss: 0.8876 - val_accuracy: 0.8892\nEpoch 382/700\n91/91 [==============================] - ETA: 0s - loss: 0.8270 - accuracy: 0.9541\nEpoch 00382: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 43s 473ms/step - loss: 0.8270 - accuracy: 0.9541 - val_loss: 0.8887 - val_accuracy: 0.8864\nEpoch 383/700\n91/91 [==============================] - ETA: 0s - loss: 0.8287 - accuracy: 0.9513\nEpoch 00383: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 55s 608ms/step - loss: 0.8287 - accuracy: 0.9513 - val_loss: 0.8841 - val_accuracy: 0.8935\nEpoch 384/700\n91/91 [==============================] - ETA: 0s - loss: 0.8262 - accuracy: 0.9569\nEpoch 00384: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 43s 475ms/step - loss: 0.8262 - accuracy: 0.9569 - val_loss: 0.8803 - val_accuracy: 0.8949\nEpoch 385/700\n91/91 [==============================] - ETA: 0s - loss: 0.8269 - accuracy: 0.9527\nEpoch 00385: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 55s 608ms/step - loss: 0.8269 - accuracy: 0.9527 - val_loss: 0.8828 - val_accuracy: 0.8892\nEpoch 386/700\n91/91 [==============================] - ETA: 0s - loss: 0.8293 - accuracy: 0.9493\nEpoch 00386: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 43s 473ms/step - loss: 0.8293 - accuracy: 0.9493 - val_loss: 0.8858 - val_accuracy: 0.8949\nEpoch 387/700\n91/91 [==============================] - ETA: 0s - loss: 0.8292 - accuracy: 0.9513\nEpoch 00387: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 55s 606ms/step - loss: 0.8292 - accuracy: 0.9513 - val_loss: 0.8848 - val_accuracy: 0.8920\nEpoch 388/700\n91/91 [==============================] - ETA: 0s - loss: 0.8267 - accuracy: 0.9558\nEpoch 00388: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 43s 473ms/step - loss: 0.8267 - accuracy: 0.9558 - val_loss: 0.8777 - val_accuracy: 0.8991\nEpoch 389/700\n91/91 [==============================] - ETA: 0s - loss: 0.8289 - accuracy: 0.9517\nEpoch 00389: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 55s 606ms/step - loss: 0.8289 - accuracy: 0.9517 - val_loss: 0.8898 - val_accuracy: 0.8821\nEpoch 390/700\n91/91 [==============================] - ETA: 0s - loss: 0.8266 - accuracy: 0.9537\nEpoch 00390: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 43s 475ms/step - loss: 0.8266 - accuracy: 0.9537 - val_loss: 0.8842 - val_accuracy: 0.8935\nEpoch 391/700\n91/91 [==============================] - ETA: 0s - loss: 0.8255 - accuracy: 0.9582\nEpoch 00391: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 55s 606ms/step - loss: 0.8255 - accuracy: 0.9582 - val_loss: 0.8870 - val_accuracy: 0.8920\nEpoch 392/700\n91/91 [==============================] - ETA: 0s - loss: 0.8259 - accuracy: 0.9551\nEpoch 00392: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 43s 473ms/step - loss: 0.8259 - accuracy: 0.9551 - val_loss: 0.8872 - val_accuracy: 0.8878\nEpoch 393/700\n91/91 [==============================] - ETA: 0s - loss: 0.8295 - accuracy: 0.9503\nEpoch 00393: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 55s 607ms/step - loss: 0.8295 - accuracy: 0.9503 - val_loss: 0.8820 - val_accuracy: 0.8935\nEpoch 394/700\n91/91 [==============================] - ETA: 0s - loss: 0.8244 - accuracy: 0.9579\nEpoch 00394: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 44s 481ms/step - loss: 0.8244 - accuracy: 0.9579 - val_loss: 0.8906 - val_accuracy: 0.8849\nEpoch 395/700\n91/91 [==============================] - ETA: 0s - loss: 0.8274 - accuracy: 0.9534\nEpoch 00395: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 55s 606ms/step - loss: 0.8274 - accuracy: 0.9534 - val_loss: 0.8800 - val_accuracy: 0.8949\nEpoch 396/700\n91/91 [==============================] - ETA: 0s - loss: 0.8251 - accuracy: 0.9555\nEpoch 00396: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 43s 473ms/step - loss: 0.8251 - accuracy: 0.9555 - val_loss: 0.8805 - val_accuracy: 0.8949\nEpoch 397/700\n91/91 [==============================] - ETA: 0s - loss: 0.8267 - accuracy: 0.9548\nEpoch 00397: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 55s 608ms/step - loss: 0.8267 - accuracy: 0.9548 - val_loss: 0.8848 - val_accuracy: 0.8892\nEpoch 398/700\n91/91 [==============================] - ETA: 0s - loss: 0.8259 - accuracy: 0.9558\nEpoch 00398: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 43s 475ms/step - loss: 0.8259 - accuracy: 0.9558 - val_loss: 0.8848 - val_accuracy: 0.8906\nEpoch 399/700\n91/91 [==============================] - ETA: 0s - loss: 0.8266 - accuracy: 0.9562\nEpoch 00399: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 55s 607ms/step - loss: 0.8266 - accuracy: 0.9562 - val_loss: 0.8836 - val_accuracy: 0.8906\nEpoch 400/700\n91/91 [==============================] - ETA: 0s - loss: 0.8281 - accuracy: 0.9534\nEpoch 00400: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 43s 475ms/step - loss: 0.8281 - accuracy: 0.9534 - val_loss: 0.8824 - val_accuracy: 0.8949\nEpoch 401/700\n91/91 [==============================] - ETA: 0s - loss: 0.8279 - accuracy: 0.9513\nEpoch 00401: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 55s 608ms/step - loss: 0.8279 - accuracy: 0.9513 - val_loss: 0.8826 - val_accuracy: 0.8906\nEpoch 402/700\n91/91 [==============================] - ETA: 0s - loss: 0.8271 - accuracy: 0.9544\nEpoch 00402: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 43s 475ms/step - loss: 0.8271 - accuracy: 0.9544 - val_loss: 0.8863 - val_accuracy: 0.8920\nEpoch 403/700\n91/91 [==============================] - ETA: 0s - loss: 0.8251 - accuracy: 0.9569\nEpoch 00403: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 55s 605ms/step - loss: 0.8251 - accuracy: 0.9569 - val_loss: 0.8824 - val_accuracy: 0.8920\nEpoch 404/700\n91/91 [==============================] - ETA: 0s - loss: 0.8272 - accuracy: 0.9541\nEpoch 00404: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 43s 476ms/step - loss: 0.8272 - accuracy: 0.9541 - val_loss: 0.8812 - val_accuracy: 0.8963\nEpoch 405/700\n91/91 [==============================] - ETA: 0s - loss: 0.8266 - accuracy: 0.9537\nEpoch 00405: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 55s 608ms/step - loss: 0.8266 - accuracy: 0.9537 - val_loss: 0.8811 - val_accuracy: 0.8906\nEpoch 406/700\n91/91 [==============================] - ETA: 0s - loss: 0.8265 - accuracy: 0.9558\nEpoch 00406: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 43s 477ms/step - loss: 0.8265 - accuracy: 0.9558 - val_loss: 0.8783 - val_accuracy: 0.8991\nEpoch 407/700\n91/91 [==============================] - ETA: 0s - loss: 0.8258 - accuracy: 0.9586\nEpoch 00407: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 55s 607ms/step - loss: 0.8258 - accuracy: 0.9586 - val_loss: 0.8806 - val_accuracy: 0.8920\nEpoch 408/700\n91/91 [==============================] - ETA: 0s - loss: 0.8247 - accuracy: 0.9565\nEpoch 00408: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 43s 476ms/step - loss: 0.8247 - accuracy: 0.9565 - val_loss: 0.8859 - val_accuracy: 0.8892\nEpoch 409/700\n91/91 [==============================] - ETA: 0s - loss: 0.8267 - accuracy: 0.9551\nEpoch 00409: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 55s 609ms/step - loss: 0.8267 - accuracy: 0.9551 - val_loss: 0.8819 - val_accuracy: 0.8991\nEpoch 410/700\n91/91 [==============================] - ETA: 0s - loss: 0.8292 - accuracy: 0.9513\nEpoch 00410: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 43s 475ms/step - loss: 0.8292 - accuracy: 0.9513 - val_loss: 0.8831 - val_accuracy: 0.8935\nEpoch 411/700\n91/91 [==============================] - ETA: 0s - loss: 0.8284 - accuracy: 0.9513\nEpoch 00411: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 55s 607ms/step - loss: 0.8284 - accuracy: 0.9513 - val_loss: 0.8810 - val_accuracy: 0.8977\nEpoch 412/700\n91/91 [==============================] - ETA: 0s - loss: 0.8268 - accuracy: 0.9523\nEpoch 00412: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 44s 485ms/step - loss: 0.8268 - accuracy: 0.9523 - val_loss: 0.8836 - val_accuracy: 0.8906\nEpoch 413/700\n91/91 [==============================] - ETA: 0s - loss: 0.8253 - accuracy: 0.9565\nEpoch 00413: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 55s 605ms/step - loss: 0.8253 - accuracy: 0.9565 - val_loss: 0.8788 - val_accuracy: 0.8977\nEpoch 414/700\n91/91 [==============================] - ETA: 0s - loss: 0.8262 - accuracy: 0.9569\nEpoch 00414: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 43s 472ms/step - loss: 0.8262 - accuracy: 0.9569 - val_loss: 0.8847 - val_accuracy: 0.8906\nEpoch 415/700\n91/91 [==============================] - ETA: 0s - loss: 0.8248 - accuracy: 0.9572\nEpoch 00415: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 56s 612ms/step - loss: 0.8248 - accuracy: 0.9572 - val_loss: 0.8804 - val_accuracy: 0.8977\nEpoch 416/700\n91/91 [==============================] - ETA: 0s - loss: 0.8279 - accuracy: 0.9527\nEpoch 00416: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 44s 479ms/step - loss: 0.8279 - accuracy: 0.9527 - val_loss: 0.8836 - val_accuracy: 0.8920\nEpoch 417/700\n91/91 [==============================] - ETA: 0s - loss: 0.8260 - accuracy: 0.9562\nEpoch 00417: val_accuracy did not improve from 0.90057\n91/91 [==============================] - 55s 608ms/step - loss: 0.8260 - accuracy: 0.9562 - val_loss: 0.8866 - val_accuracy: 0.8920\nEpoch 418/700\n91/91 [==============================] - ETA: 0s - loss: 0.8268 - accuracy: 0.9555\nEpoch 00418: val_accuracy did not improve from 0.90057\nRestoring model weights from the end of the best epoch.\n91/91 [==============================] - 43s 473ms/step - loss: 0.8268 - accuracy: 0.9555 - val_loss: 0.8815 - val_accuracy: 0.8963\nEpoch 00418: early stopping\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x2383bda1dc0>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "import time\n",
    "!rm -rf logs\n",
    "time.sleep(2)\n",
    "\n",
    "model.fit(\n",
    "    training_data_generator,\n",
    "    epochs=700,\n",
    "    verbose=True,\n",
    "    steps_per_epoch=training_data_generator.samples // \n",
    "        training_data_generator.batch_size,\n",
    "    validation_data=validation_data_generator,\n",
    "    validation_steps=validation_data_generator.samples // \n",
    "        validation_data_generator.batch_size,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            'isthemountainout.best.h5',\n",
    "            monitor='val_accuracy',\n",
    "            mode='max',\n",
    "            save_best_only=True,\n",
    "            verbose=True),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            patience=50,\n",
    "            restore_best_weights=True,\n",
    "            verbose=True),\n",
    "        tf.keras.callbacks.CSVLogger(os.path.join('logs', 'isthemountainout.training.csv')),\n",
    "        tf.keras.callbacks.TensorBoard(\n",
    "            log_dir='logs', update_freq='batch', write_images=True, write_graph=True, embeddings_freq=10),\n",
    "        m.LogConfusionMatrixCallback(\n",
    "            model=model,\n",
    "            datagen=tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0 / 255)\\\n",
    "                .flow_from_directory(data_directory, batch_size=3096, shuffle=True, target_size=image_size),\n",
    "            logdir=os.path.join('logs', 'image'))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python_defaultSpec_1601353642675",
   "display_name": "Python 3.8.5 64-bit ('tf_gpu': conda)",
   "metadata": {
    "interpreter": {
     "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}