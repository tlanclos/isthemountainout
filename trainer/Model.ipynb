{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6086 images belonging to 4 classes.\n",
      "Found 1521 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "%run GpuOptions.ipynb\n",
    "%run BuildTrainingData.ipynb\n",
    "%run TrendTools.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 112, 112, 32) 896         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 112, 112, 32) 128         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 112, 112, 32) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 112, 112, 64) 18496       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 112, 112, 64) 256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 112, 112, 64) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 112, 112, 64) 0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_2 (SeparableCo (None, 112, 112, 128 8896        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 112, 112, 128 512         separable_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 112, 112, 128 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_3 (SeparableCo (None, 112, 112, 128 17664       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 112, 112, 128 512         separable_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 56, 56, 128)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 56, 56, 128)  8320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 56, 56, 128)  0           max_pooling2d[0][0]              \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 56, 56, 128)  0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_4 (SeparableCo (None, 56, 56, 256)  34176       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 56, 56, 256)  1024        separable_conv2d_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 56, 56, 256)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_5 (SeparableCo (None, 56, 56, 256)  68096       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 56, 56, 256)  1024        separable_conv2d_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 28, 28, 256)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 28, 28, 256)  33024       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 28, 28, 256)  0           max_pooling2d_1[0][0]            \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 28, 28, 256)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_6 (SeparableCo (None, 28, 28, 728)  189400      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 28, 28, 728)  2912        separable_conv2d_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 28, 28, 728)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_7 (SeparableCo (None, 28, 28, 728)  537264      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 28, 28, 728)  2912        separable_conv2d_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 14, 14, 728)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 14, 14, 728)  187096      add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 14, 14, 728)  0           max_pooling2d_2[0][0]            \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 14, 14, 728)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_8 (SeparableCo (None, 14, 14, 728)  537264      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 14, 14, 728)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_9 (SeparableCo (None, 14, 14, 728)  537264      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 14, 14, 728)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_10 (SeparableC (None, 14, 14, 728)  537264      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 14, 14, 728)  0           batch_normalization_12[0][0]     \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 14, 14, 728)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_11 (SeparableC (None, 14, 14, 728)  537264      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 14, 14, 728)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_12 (SeparableC (None, 14, 14, 728)  537264      activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 14, 14, 728)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_13 (SeparableC (None, 14, 14, 728)  537264      activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 14, 14, 728)  0           batch_normalization_15[0][0]     \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 14, 14, 728)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_14 (SeparableC (None, 14, 14, 728)  537264      activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 14, 14, 728)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_15 (SeparableC (None, 14, 14, 728)  537264      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_15[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 14, 14, 728)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_16 (SeparableC (None, 14, 14, 728)  537264      activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_16[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 14, 14, 728)  0           batch_normalization_18[0][0]     \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 14, 14, 728)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_17 (SeparableC (None, 14, 14, 728)  537264      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_17[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 14, 14, 728)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_18 (SeparableC (None, 14, 14, 728)  537264      activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_18[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 14, 14, 728)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_19 (SeparableC (None, 14, 14, 728)  537264      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_19[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 14, 14, 728)  0           batch_normalization_21[0][0]     \n",
      "                                                                 add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 14, 14, 728)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_20 (SeparableC (None, 14, 14, 728)  537264      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_20[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 14, 14, 728)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_21 (SeparableC (None, 14, 14, 728)  537264      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_21[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 14, 14, 728)  0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_22 (SeparableC (None, 14, 14, 728)  537264      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_22[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 14, 14, 728)  0           batch_normalization_24[0][0]     \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 14, 14, 728)  0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_23 (SeparableC (None, 14, 14, 728)  537264      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_23[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 14, 14, 728)  0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_24 (SeparableC (None, 14, 14, 728)  537264      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_24[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 14, 14, 728)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_25 (SeparableC (None, 14, 14, 728)  537264      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_25[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 14, 14, 728)  0           batch_normalization_27[0][0]     \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 14, 14, 728)  0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_26 (SeparableC (None, 14, 14, 728)  537264      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_26[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 14, 14, 728)  0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_27 (SeparableC (None, 14, 14, 728)  537264      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_27[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 14, 14, 728)  0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_28 (SeparableC (None, 14, 14, 728)  537264      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_28[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 14, 14, 728)  0           batch_normalization_30[0][0]     \n",
      "                                                                 add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 14, 14, 728)  0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_29 (SeparableC (None, 14, 14, 728)  537264      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_29[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 14, 14, 728)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_30 (SeparableC (None, 14, 14, 728)  537264      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_30[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 14, 14, 728)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_31 (SeparableC (None, 14, 14, 728)  537264      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_31[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 14, 14, 728)  0           batch_normalization_33[0][0]     \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 14, 14, 728)  0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_32 (SeparableC (None, 14, 14, 728)  537264      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 14, 14, 728)  2912        separable_conv2d_32[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 14, 14, 728)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_33 (SeparableC (None, 14, 14, 1024) 753048      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 14, 14, 1024) 4096        separable_conv2d_33[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 7, 7, 1024)   0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 7, 7, 1024)   746496      add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 7, 7, 1024)   0           max_pooling2d_3[0][0]            \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 7, 7, 1024)   0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d (SeparableConv (None, 7, 7, 728)    755416      activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 7, 7, 728)    2912        separable_conv2d[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 7, 7, 728)    0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_1 (SeparableCo (None, 7, 7, 1024)   753048      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 7, 7, 1024)   4096        separable_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 1024)         0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 4)            4100        global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 4)            0           dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 17,640,220\n",
      "Trainable params: 17,593,628\n",
      "Non-trainable params: 46,592\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "import common.model as m\n",
    "\n",
    "classes = training_data_generator.class_indices\n",
    "data, _ = training_data_generator.next()\n",
    "shape = data[0].shape\n",
    "inputs = tf.keras.Input(shape=shape)\n",
    "\n",
    "outputs = m.chained(\n",
    "    # Entry Flow\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    m.expand(\n",
    "        flow=lambda previous_activation, size: tf.keras.layers.add([\n",
    "            m.chained(\n",
    "                m.duplicate(\n",
    "                    layers=lambda: [\n",
    "                        tf.keras.layers.Activation('relu'),\n",
    "                        tf.keras.layers.SeparableConv2D(filters=size, kernel_size=3, padding='same'),\n",
    "                        tf.keras.layers.BatchNormalization(),\n",
    "                    ],\n",
    "                    count=2\n",
    "                ),\n",
    "                tf.keras.layers.MaxPooling2D(pool_size=3, strides=2, padding='same'),\n",
    "            )(previous_activation),\n",
    "            tf.keras.layers.Conv2D(filters=size, kernel_size=1, strides=2, padding='same')(previous_activation),\n",
    "        ]),\n",
    "        values=[128, 256, 728],\n",
    "    ),\n",
    "\n",
    "    # # Middle Flow\n",
    "    m.expand(\n",
    "        flow=lambda previous_activation, _: tf.keras.layers.add([\n",
    "            m.duplicate(\n",
    "                layers=lambda: [\n",
    "                    tf.keras.layers.Activation('relu'),\n",
    "                    tf.keras.layers.SeparableConv2D(filters=728, kernel_size=3, padding='same'),\n",
    "                    tf.keras.layers.BatchNormalization(),\n",
    "                ],\n",
    "                count=3,\n",
    "            )(previous_activation),\n",
    "            previous_activation,\n",
    "        ]),\n",
    "        values=[0] * 8\n",
    "    ),\n",
    "\n",
    "    # # Exit Flow\n",
    "    lambda previous_activation: tf.keras.layers.add([\n",
    "        m.chained(\n",
    "            tf.keras.layers.Activation('relu'),\n",
    "            tf.keras.layers.SeparableConv2D(filters=728, kernel_size=3, padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Activation('relu'),\n",
    "            tf.keras.layers.SeparableConv2D(filters=1024, kernel_size=3, padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=3, strides=2, padding='same'),\n",
    "        )(previous_activation),\n",
    "        tf.keras.layers.Conv2D(filters=1024, kernel_size=1, strides=2, padding='same')(previous_activation),\n",
    "    ]),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.SeparableConv2D(filters=728, kernel_size=3, padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.SeparableConv2D(filters=1024, kernel_size=3, padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(len(classes), activation='linear'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    ")(inputs)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.build(input_shape=(None, *shape))\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adadelta(learning_rate=0.008),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(\n",
    "        from_logits=True, label_smoothing=0.1),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "if os.path.exists('isthemountainout.best.h5'):\n",
    "    model.load_weights('isthemountainout.best.h5')\n",
    "elif os.path.exists('isthemountainout.h5'):\n",
    "    model.load_weights('isthemountainout.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7607 images belonging to 4 classes.\n",
      "Epoch 1/700\n",
      "   1/1521 [..............................] - ETA: 0s - loss: 1.0066 - accuracy: 0.5000WARNING:tensorflow:From C:\\Users\\Taylor\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "   2/1521 [..............................] - ETA: 2:16 - loss: 1.0495 - accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0370s vs `on_train_batch_end` time: 0.1414s). Check your callbacks.\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.6240 - accuracy: 0.8588\n",
      "Epoch 00001: val_loss improved from inf to 0.56974, saving model to isthemountainout.best.h5\n",
      "1521/1521 [==============================] - 179s 118ms/step - loss: 0.6240 - accuracy: 0.8588 - val_loss: 0.5697 - val_accuracy: 0.8947\n",
      "Epoch 2/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.6018 - accuracy: 0.8678\n",
      "Epoch 00002: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 187s 123ms/step - loss: 0.6018 - accuracy: 0.8678 - val_loss: 0.5803 - val_accuracy: 0.8934\n",
      "Epoch 3/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5876 - accuracy: 0.8745\n",
      "Epoch 00003: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 176s 116ms/step - loss: 0.5876 - accuracy: 0.8745 - val_loss: 0.5706 - val_accuracy: 0.8947\n",
      "Epoch 4/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5807 - accuracy: 0.8793\n",
      "Epoch 00004: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 183s 120ms/step - loss: 0.5807 - accuracy: 0.8793 - val_loss: 0.5878 - val_accuracy: 0.8842\n",
      "Epoch 5/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5822 - accuracy: 0.8777\n",
      "Epoch 00005: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 183s 121ms/step - loss: 0.5822 - accuracy: 0.8777 - val_loss: 0.6199 - val_accuracy: 0.8711\n",
      "Epoch 6/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5723 - accuracy: 0.8795\n",
      "Epoch 00006: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 176s 116ms/step - loss: 0.5723 - accuracy: 0.8795 - val_loss: 0.5736 - val_accuracy: 0.8934\n",
      "Epoch 7/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5651 - accuracy: 0.8829\n",
      "Epoch 00007: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 185s 122ms/step - loss: 0.5651 - accuracy: 0.8829 - val_loss: 0.5837 - val_accuracy: 0.8921\n",
      "Epoch 8/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5679 - accuracy: 0.8847\n",
      "Epoch 00008: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 184s 121ms/step - loss: 0.5679 - accuracy: 0.8847 - val_loss: 0.6017 - val_accuracy: 0.8901\n",
      "Epoch 9/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5649 - accuracy: 0.8780\n",
      "Epoch 00009: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 176s 116ms/step - loss: 0.5649 - accuracy: 0.8780 - val_loss: 0.6277 - val_accuracy: 0.8796\n",
      "Epoch 10/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5589 - accuracy: 0.8867\n",
      "Epoch 00010: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 184s 121ms/step - loss: 0.5589 - accuracy: 0.8867 - val_loss: 0.5959 - val_accuracy: 0.8816\n",
      "Epoch 11/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5487 - accuracy: 0.8930\n",
      "Epoch 00011: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 188s 124ms/step - loss: 0.5487 - accuracy: 0.8930 - val_loss: 0.6625 - val_accuracy: 0.8724\n",
      "Epoch 12/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5527 - accuracy: 0.8875\n",
      "Epoch 00012: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 176s 116ms/step - loss: 0.5527 - accuracy: 0.8875 - val_loss: 0.6455 - val_accuracy: 0.8750\n",
      "Epoch 13/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5481 - accuracy: 0.8905\n",
      "Epoch 00013: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 185s 122ms/step - loss: 0.5481 - accuracy: 0.8905 - val_loss: 0.6506 - val_accuracy: 0.8618\n",
      "Epoch 14/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5485 - accuracy: 0.8930\n",
      "Epoch 00014: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 186s 122ms/step - loss: 0.5485 - accuracy: 0.8930 - val_loss: 0.6575 - val_accuracy: 0.8599\n",
      "Epoch 15/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5528 - accuracy: 0.8910\n",
      "Epoch 00015: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 176s 116ms/step - loss: 0.5528 - accuracy: 0.8910 - val_loss: 0.6842 - val_accuracy: 0.8500\n",
      "Epoch 16/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5426 - accuracy: 0.8935\n",
      "Epoch 00016: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 187s 123ms/step - loss: 0.5426 - accuracy: 0.8935 - val_loss: 0.7240 - val_accuracy: 0.8526\n",
      "Epoch 17/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5542 - accuracy: 0.8882\n",
      "Epoch 00017: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 200s 131ms/step - loss: 0.5542 - accuracy: 0.8882 - val_loss: 0.6910 - val_accuracy: 0.8599\n",
      "Epoch 18/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5460 - accuracy: 0.8986\n",
      "Epoch 00018: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 179s 118ms/step - loss: 0.5460 - accuracy: 0.8986 - val_loss: 0.6571 - val_accuracy: 0.8658\n",
      "Epoch 19/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5473 - accuracy: 0.8936\n",
      "Epoch 00019: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 194s 128ms/step - loss: 0.5473 - accuracy: 0.8936 - val_loss: 0.7270 - val_accuracy: 0.8500\n",
      "Epoch 20/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5356 - accuracy: 0.8999\n",
      "Epoch 00020: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 190s 125ms/step - loss: 0.5356 - accuracy: 0.8999 - val_loss: 0.6473 - val_accuracy: 0.8770\n",
      "Epoch 21/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5398 - accuracy: 0.9004\n",
      "Epoch 00021: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 178s 117ms/step - loss: 0.5398 - accuracy: 0.9004 - val_loss: 0.6815 - val_accuracy: 0.8579\n",
      "Epoch 22/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5420 - accuracy: 0.8959\n",
      "Epoch 00022: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 182s 120ms/step - loss: 0.5420 - accuracy: 0.8959 - val_loss: 0.7479 - val_accuracy: 0.8520\n",
      "Epoch 23/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5335 - accuracy: 0.9010\n",
      "Epoch 00023: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 185s 122ms/step - loss: 0.5335 - accuracy: 0.9010 - val_loss: 0.6693 - val_accuracy: 0.8776\n",
      "Epoch 24/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5379 - accuracy: 0.9004\n",
      "Epoch 00024: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 175s 115ms/step - loss: 0.5379 - accuracy: 0.9004 - val_loss: 0.7025 - val_accuracy: 0.8730\n",
      "Epoch 25/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5537 - accuracy: 0.8877\n",
      "Epoch 00025: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 190s 125ms/step - loss: 0.5537 - accuracy: 0.8877 - val_loss: 0.7330 - val_accuracy: 0.8572\n",
      "Epoch 26/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5366 - accuracy: 0.8984\n",
      "Epoch 00026: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 184s 121ms/step - loss: 0.5366 - accuracy: 0.8984 - val_loss: 0.7573 - val_accuracy: 0.8421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5454 - accuracy: 0.8931\n",
      "Epoch 00027: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 174s 114ms/step - loss: 0.5454 - accuracy: 0.8931 - val_loss: 0.6870 - val_accuracy: 0.8625\n",
      "Epoch 28/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5344 - accuracy: 0.8964\n",
      "Epoch 00028: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 184s 121ms/step - loss: 0.5344 - accuracy: 0.8964 - val_loss: 0.7128 - val_accuracy: 0.8605\n",
      "Epoch 29/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5376 - accuracy: 0.8963\n",
      "Epoch 00029: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 184s 121ms/step - loss: 0.5376 - accuracy: 0.8963 - val_loss: 0.7028 - val_accuracy: 0.8684\n",
      "Epoch 30/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5279 - accuracy: 0.9051\n",
      "Epoch 00030: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 168s 111ms/step - loss: 0.5279 - accuracy: 0.9051 - val_loss: 0.7644 - val_accuracy: 0.8500\n",
      "Epoch 31/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5332 - accuracy: 0.8994\n",
      "Epoch 00031: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 179s 117ms/step - loss: 0.5332 - accuracy: 0.8994 - val_loss: 0.7390 - val_accuracy: 0.8579\n",
      "Epoch 32/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5376 - accuracy: 0.8989\n",
      "Epoch 00032: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 177s 116ms/step - loss: 0.5376 - accuracy: 0.8989 - val_loss: 0.7257 - val_accuracy: 0.8553\n",
      "Epoch 33/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5355 - accuracy: 0.8976\n",
      "Epoch 00033: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 168s 110ms/step - loss: 0.5355 - accuracy: 0.8976 - val_loss: 0.6471 - val_accuracy: 0.8796\n",
      "Epoch 34/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5322 - accuracy: 0.8967\n",
      "Epoch 00034: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 179s 118ms/step - loss: 0.5322 - accuracy: 0.8967 - val_loss: 0.6939 - val_accuracy: 0.8711\n",
      "Epoch 35/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5291 - accuracy: 0.9030\n",
      "Epoch 00035: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 175s 115ms/step - loss: 0.5291 - accuracy: 0.9030 - val_loss: 0.7196 - val_accuracy: 0.8625\n",
      "Epoch 36/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5246 - accuracy: 0.9048\n",
      "Epoch 00036: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 167s 110ms/step - loss: 0.5246 - accuracy: 0.9048 - val_loss: 0.6910 - val_accuracy: 0.8711\n",
      "Epoch 37/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5264 - accuracy: 0.9015\n",
      "Epoch 00037: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 176s 116ms/step - loss: 0.5264 - accuracy: 0.9015 - val_loss: 0.6835 - val_accuracy: 0.8711\n",
      "Epoch 38/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5243 - accuracy: 0.9064\n",
      "Epoch 00038: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 182s 120ms/step - loss: 0.5243 - accuracy: 0.9064 - val_loss: 0.6771 - val_accuracy: 0.8691\n",
      "Epoch 39/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5321 - accuracy: 0.8979\n",
      "Epoch 00039: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 169s 111ms/step - loss: 0.5321 - accuracy: 0.8979 - val_loss: 0.6741 - val_accuracy: 0.8803\n",
      "Epoch 40/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5360 - accuracy: 0.8989\n",
      "Epoch 00040: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 177s 116ms/step - loss: 0.5360 - accuracy: 0.8989 - val_loss: 0.7033 - val_accuracy: 0.8658\n",
      "Epoch 41/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5284 - accuracy: 0.9035\n",
      "Epoch 00041: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 178s 117ms/step - loss: 0.5284 - accuracy: 0.9035 - val_loss: 0.6753 - val_accuracy: 0.8697\n",
      "Epoch 42/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5278 - accuracy: 0.8966\n",
      "Epoch 00042: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 176s 116ms/step - loss: 0.5278 - accuracy: 0.8966 - val_loss: 0.5962 - val_accuracy: 0.8901\n",
      "Epoch 43/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5345 - accuracy: 0.9015\n",
      "Epoch 00043: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 182s 120ms/step - loss: 0.5345 - accuracy: 0.9015 - val_loss: 0.6735 - val_accuracy: 0.8763\n",
      "Epoch 44/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5327 - accuracy: 0.8954\n",
      "Epoch 00044: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 187s 123ms/step - loss: 0.5327 - accuracy: 0.8954 - val_loss: 0.7200 - val_accuracy: 0.8579\n",
      "Epoch 45/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5356 - accuracy: 0.9000\n",
      "Epoch 00045: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 176s 116ms/step - loss: 0.5356 - accuracy: 0.9000 - val_loss: 0.6598 - val_accuracy: 0.8691\n",
      "Epoch 46/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5265 - accuracy: 0.9027\n",
      "Epoch 00046: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 181s 119ms/step - loss: 0.5265 - accuracy: 0.9027 - val_loss: 0.7451 - val_accuracy: 0.8467\n",
      "Epoch 47/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5271 - accuracy: 0.8994\n",
      "Epoch 00047: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 185s 122ms/step - loss: 0.5271 - accuracy: 0.8994 - val_loss: 0.8212 - val_accuracy: 0.8276\n",
      "Epoch 48/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5300 - accuracy: 0.8984\n",
      "Epoch 00048: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 170s 112ms/step - loss: 0.5300 - accuracy: 0.8984 - val_loss: 0.6887 - val_accuracy: 0.8737\n",
      "Epoch 49/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5201 - accuracy: 0.9025\n",
      "Epoch 00049: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 179s 117ms/step - loss: 0.5201 - accuracy: 0.9025 - val_loss: 0.7167 - val_accuracy: 0.8553\n",
      "Epoch 50/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5278 - accuracy: 0.8990\n",
      "Epoch 00050: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 183s 121ms/step - loss: 0.5278 - accuracy: 0.8990 - val_loss: 0.6823 - val_accuracy: 0.8730\n",
      "Epoch 51/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5299 - accuracy: 0.8961\n",
      "Epoch 00051: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 178s 117ms/step - loss: 0.5299 - accuracy: 0.8961 - val_loss: 0.6502 - val_accuracy: 0.8750\n",
      "Epoch 52/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5280 - accuracy: 0.8995\n",
      "Epoch 00052: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 183s 120ms/step - loss: 0.5280 - accuracy: 0.8995 - val_loss: 0.6905 - val_accuracy: 0.8599\n",
      "Epoch 53/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5220 - accuracy: 0.9037\n",
      "Epoch 00053: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 176s 116ms/step - loss: 0.5220 - accuracy: 0.9037 - val_loss: 0.7322 - val_accuracy: 0.8632\n",
      "Epoch 54/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5252 - accuracy: 0.9043\n",
      "Epoch 00054: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 165s 109ms/step - loss: 0.5252 - accuracy: 0.9043 - val_loss: 0.6430 - val_accuracy: 0.8849\n",
      "Epoch 55/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5262 - accuracy: 0.9051\n",
      "Epoch 00055: val_loss did not improve from 0.56974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1521/1521 [==============================] - 174s 114ms/step - loss: 0.5262 - accuracy: 0.9051 - val_loss: 0.6875 - val_accuracy: 0.8671\n",
      "Epoch 56/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5265 - accuracy: 0.9022\n",
      "Epoch 00056: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 174s 114ms/step - loss: 0.5265 - accuracy: 0.9022 - val_loss: 0.6756 - val_accuracy: 0.8684\n",
      "Epoch 57/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5148 - accuracy: 0.9107\n",
      "Epoch 00057: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 165s 108ms/step - loss: 0.5148 - accuracy: 0.9107 - val_loss: 0.6582 - val_accuracy: 0.8783\n",
      "Epoch 58/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5151 - accuracy: 0.9129\n",
      "Epoch 00058: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 173s 114ms/step - loss: 0.5151 - accuracy: 0.9129 - val_loss: 0.7058 - val_accuracy: 0.8612\n",
      "Epoch 59/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5237 - accuracy: 0.8959\n",
      "Epoch 00059: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 171s 112ms/step - loss: 0.5237 - accuracy: 0.8959 - val_loss: 0.6754 - val_accuracy: 0.8697\n",
      "Epoch 60/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5208 - accuracy: 0.9017\n",
      "Epoch 00060: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 160s 105ms/step - loss: 0.5208 - accuracy: 0.9017 - val_loss: 0.7551 - val_accuracy: 0.8507\n",
      "Epoch 61/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5245 - accuracy: 0.8992\n",
      "Epoch 00061: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 170s 112ms/step - loss: 0.5245 - accuracy: 0.8992 - val_loss: 0.7200 - val_accuracy: 0.8586\n",
      "Epoch 62/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5229 - accuracy: 0.9055\n",
      "Epoch 00062: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 169s 111ms/step - loss: 0.5229 - accuracy: 0.9055 - val_loss: 0.6909 - val_accuracy: 0.8671\n",
      "Epoch 63/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5206 - accuracy: 0.9058\n",
      "Epoch 00063: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 161s 106ms/step - loss: 0.5206 - accuracy: 0.9058 - val_loss: 0.6484 - val_accuracy: 0.8757\n",
      "Epoch 64/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5234 - accuracy: 0.9041\n",
      "Epoch 00064: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 170s 112ms/step - loss: 0.5234 - accuracy: 0.9041 - val_loss: 0.7118 - val_accuracy: 0.8678\n",
      "Epoch 65/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5165 - accuracy: 0.9074\n",
      "Epoch 00065: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 169s 111ms/step - loss: 0.5165 - accuracy: 0.9074 - val_loss: 0.6843 - val_accuracy: 0.8664\n",
      "Epoch 66/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5256 - accuracy: 0.8976\n",
      "Epoch 00066: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 163s 107ms/step - loss: 0.5256 - accuracy: 0.8976 - val_loss: 0.6984 - val_accuracy: 0.8691\n",
      "Epoch 67/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5234 - accuracy: 0.9045\n",
      "Epoch 00067: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 167s 110ms/step - loss: 0.5234 - accuracy: 0.9045 - val_loss: 0.6939 - val_accuracy: 0.8724\n",
      "Epoch 68/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5235 - accuracy: 0.9009\n",
      "Epoch 00068: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 169s 111ms/step - loss: 0.5235 - accuracy: 0.9009 - val_loss: 0.7359 - val_accuracy: 0.8638\n",
      "Epoch 69/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5257 - accuracy: 0.8990\n",
      "Epoch 00069: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 161s 106ms/step - loss: 0.5257 - accuracy: 0.8990 - val_loss: 0.7188 - val_accuracy: 0.8678\n",
      "Epoch 70/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5260 - accuracy: 0.8974\n",
      "Epoch 00070: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 168s 111ms/step - loss: 0.5260 - accuracy: 0.8974 - val_loss: 0.7334 - val_accuracy: 0.8651\n",
      "Epoch 71/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5302 - accuracy: 0.8990\n",
      "Epoch 00071: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 169s 111ms/step - loss: 0.5302 - accuracy: 0.8990 - val_loss: 0.7137 - val_accuracy: 0.8586\n",
      "Epoch 72/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5212 - accuracy: 0.9028\n",
      "Epoch 00072: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 160s 105ms/step - loss: 0.5212 - accuracy: 0.9028 - val_loss: 0.7346 - val_accuracy: 0.8638\n",
      "Epoch 73/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5293 - accuracy: 0.9002\n",
      "Epoch 00073: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 169s 111ms/step - loss: 0.5293 - accuracy: 0.9002 - val_loss: 0.7286 - val_accuracy: 0.8711\n",
      "Epoch 74/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5331 - accuracy: 0.8912\n",
      "Epoch 00074: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 168s 111ms/step - loss: 0.5331 - accuracy: 0.8912 - val_loss: 0.7516 - val_accuracy: 0.8599\n",
      "Epoch 75/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5229 - accuracy: 0.8966\n",
      "Epoch 00075: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 160s 105ms/step - loss: 0.5229 - accuracy: 0.8966 - val_loss: 0.7128 - val_accuracy: 0.8651\n",
      "Epoch 76/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5272 - accuracy: 0.8931\n",
      "Epoch 00076: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 168s 110ms/step - loss: 0.5272 - accuracy: 0.8931 - val_loss: 0.7108 - val_accuracy: 0.8737\n",
      "Epoch 77/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5122 - accuracy: 0.9060\n",
      "Epoch 00077: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 168s 110ms/step - loss: 0.5122 - accuracy: 0.9060 - val_loss: 0.6900 - val_accuracy: 0.8724\n",
      "Epoch 78/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5309 - accuracy: 0.8949\n",
      "Epoch 00078: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 161s 106ms/step - loss: 0.5309 - accuracy: 0.8949 - val_loss: 0.7162 - val_accuracy: 0.8763\n",
      "Epoch 79/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5261 - accuracy: 0.8961\n",
      "Epoch 00079: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 168s 111ms/step - loss: 0.5261 - accuracy: 0.8961 - val_loss: 0.7560 - val_accuracy: 0.8572\n",
      "Epoch 80/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5241 - accuracy: 0.9005\n",
      "Epoch 00080: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 168s 111ms/step - loss: 0.5241 - accuracy: 0.9005 - val_loss: 0.6812 - val_accuracy: 0.8868\n",
      "Epoch 81/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5294 - accuracy: 0.8964\n",
      "Epoch 00081: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 161s 106ms/step - loss: 0.5294 - accuracy: 0.8964 - val_loss: 0.7009 - val_accuracy: 0.8691\n",
      "Epoch 82/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5222 - accuracy: 0.8984\n",
      "Epoch 00082: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 168s 110ms/step - loss: 0.5222 - accuracy: 0.8984 - val_loss: 0.7195 - val_accuracy: 0.8625\n",
      "Epoch 83/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5185 - accuracy: 0.9061\n",
      "Epoch 00083: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 168s 111ms/step - loss: 0.5185 - accuracy: 0.9061 - val_loss: 0.6960 - val_accuracy: 0.8763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5221 - accuracy: 0.9074\n",
      "Epoch 00084: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 160s 105ms/step - loss: 0.5221 - accuracy: 0.9074 - val_loss: 0.7657 - val_accuracy: 0.8546\n",
      "Epoch 85/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5207 - accuracy: 0.9047\n",
      "Epoch 00085: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 168s 111ms/step - loss: 0.5207 - accuracy: 0.9047 - val_loss: 0.6865 - val_accuracy: 0.8763\n",
      "Epoch 86/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5235 - accuracy: 0.9018\n",
      "Epoch 00086: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 169s 111ms/step - loss: 0.5235 - accuracy: 0.9018 - val_loss: 0.7262 - val_accuracy: 0.8651\n",
      "Epoch 87/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5195 - accuracy: 0.9013\n",
      "Epoch 00087: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 160s 105ms/step - loss: 0.5195 - accuracy: 0.9013 - val_loss: 0.6936 - val_accuracy: 0.8750\n",
      "Epoch 88/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5150 - accuracy: 0.9066\n",
      "Epoch 00088: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 168s 110ms/step - loss: 0.5150 - accuracy: 0.9066 - val_loss: 0.6760 - val_accuracy: 0.8796\n",
      "Epoch 89/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5256 - accuracy: 0.8949\n",
      "Epoch 00089: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 169s 111ms/step - loss: 0.5256 - accuracy: 0.8949 - val_loss: 0.6820 - val_accuracy: 0.8776\n",
      "Epoch 90/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5225 - accuracy: 0.9058\n",
      "Epoch 00090: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 160s 105ms/step - loss: 0.5225 - accuracy: 0.9058 - val_loss: 0.7433 - val_accuracy: 0.8678\n",
      "Epoch 91/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5186 - accuracy: 0.8982\n",
      "Epoch 00091: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 169s 111ms/step - loss: 0.5186 - accuracy: 0.8982 - val_loss: 0.6692 - val_accuracy: 0.8855\n",
      "Epoch 92/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5193 - accuracy: 0.9055\n",
      "Epoch 00092: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 168s 110ms/step - loss: 0.5193 - accuracy: 0.9055 - val_loss: 0.6927 - val_accuracy: 0.8770\n",
      "Epoch 93/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5239 - accuracy: 0.8956\n",
      "Epoch 00093: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 160s 105ms/step - loss: 0.5239 - accuracy: 0.8956 - val_loss: 0.7122 - val_accuracy: 0.8697\n",
      "Epoch 94/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5174 - accuracy: 0.9071\n",
      "Epoch 00094: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 168s 110ms/step - loss: 0.5174 - accuracy: 0.9071 - val_loss: 0.7519 - val_accuracy: 0.8691\n",
      "Epoch 95/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5257 - accuracy: 0.8964\n",
      "Epoch 00095: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 168s 111ms/step - loss: 0.5257 - accuracy: 0.8964 - val_loss: 0.7489 - val_accuracy: 0.8579\n",
      "Epoch 96/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5294 - accuracy: 0.8941\n",
      "Epoch 00096: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 161s 106ms/step - loss: 0.5294 - accuracy: 0.8941 - val_loss: 0.7814 - val_accuracy: 0.8454\n",
      "Epoch 97/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5194 - accuracy: 0.9064\n",
      "Epoch 00097: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 167s 110ms/step - loss: 0.5194 - accuracy: 0.9064 - val_loss: 0.7619 - val_accuracy: 0.8612\n",
      "Epoch 98/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5272 - accuracy: 0.8944\n",
      "Epoch 00098: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 169s 111ms/step - loss: 0.5272 - accuracy: 0.8944 - val_loss: 0.7074 - val_accuracy: 0.8711\n",
      "Epoch 99/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5239 - accuracy: 0.8987\n",
      "Epoch 00099: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 160s 105ms/step - loss: 0.5239 - accuracy: 0.8987 - val_loss: 0.6381 - val_accuracy: 0.8882\n",
      "Epoch 100/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5199 - accuracy: 0.9048\n",
      "Epoch 00100: val_loss did not improve from 0.56974\n",
      "1521/1521 [==============================] - 168s 110ms/step - loss: 0.5199 - accuracy: 0.9048 - val_loss: 0.6839 - val_accuracy: 0.8757\n",
      "Epoch 101/700\n",
      "1521/1521 [==============================] - ETA: 0s - loss: 0.5178 - accuracy: 0.9071\n",
      "Epoch 00101: val_loss did not improve from 0.56974\n",
      "Restoring model weights from the end of the best epoch.\n",
      "1521/1521 [==============================] - 170s 112ms/step - loss: 0.5178 - accuracy: 0.9071 - val_loss: 0.7570 - val_accuracy: 0.8645\n",
      "Epoch 00101: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ae53edb130>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "model.fit(\n",
    "    training_data_generator,\n",
    "    epochs=700,\n",
    "    verbose=True,\n",
    "    steps_per_epoch=training_data_generator.samples // \n",
    "        training_data_generator.batch_size,\n",
    "    validation_data=validation_data_generator,\n",
    "    validation_steps=validation_data_generator.samples // \n",
    "        validation_data_generator.batch_size,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            'isthemountainout.best.h5',\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_best_only=True,\n",
    "            verbose=True),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            patience=100,\n",
    "            restore_best_weights=True,\n",
    "            verbose=True),\n",
    "        # tf.keras.callbacks.CSVLogger(os.path.join('logs', 'isthemountainout.training.csv')),\n",
    "        tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=os.path.join('logs', 'fit', datetime.now().strftime('%Y%m%d%H%M%S')),\n",
    "            update_freq=50,\n",
    "            write_images=True,\n",
    "            write_graph=True,\n",
    "            embeddings_freq=10),\n",
    "        m.LogConfusionMatrixCallback(\n",
    "            model=model,\n",
    "            datagen=tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0 / 255)\\\n",
    "                .flow_from_directory(data_directory, batch_size=3096, shuffle=True, target_size=image_size),\n",
    "            logdir=os.path.join('logs', 'image'))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
